{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applying DP to New York citibike data\n",
    "\n",
    "Here's a new demonstration of the methods described in our DP for GP paper. We use the citibike data.\n",
    "\n",
    "We'll bin the data into a histogram, with two axes: Time since start of week and Duration of bike ride.\n",
    "\n",
    "## Results Summary\n",
    "\n",
    "### Integral vs RBF kernel\n",
    "  \n",
    "I've found that the RBF kernel doesn't seem to converge properly: FIXED by applying the square-root transform to the data prior to fitting - makes it a bit more normal, and GPy seems to cope fine now.\n",
    "\n",
    "    Normal GP,   RMSE 18.444\n",
    "    Pseudo GP,   RMSE 22.227\n",
    "    Normal GP DP,RMSE 63.233 (0.085)\n",
    "    Pseudo GP DP,RMSE 22.657 (0.009)\n",
    "\n",
    "### Inducing inputs\n",
    "\n",
    "Inducing inputs help lots, but it does depend on the scale of the histogram grid and the distribution of the inducing inputs.\n",
    "\n",
    "\n",
    "\n",
    "### Maybe\n",
    "\n",
    "It seems that maybe my inducing inputs+DP does EVEN BETTER than the standard histogram with noise added. This is quite exciting as it may mean there's a new better way of applying DP to such data. I.e. we've come up with a better way of applying DP! I basically assume I'm wrong here and have made a mistake, as that result would be too good :).\n",
    "\n",
    "## Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "full_df = pandas.read_csv('201606-citibike-tripdata.csv')#,nrows=1000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = full_df[full_df['start station id']==300].copy() #380"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate 'hours since start of week'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finds the number of seconds since the start of the week (see 'seconds') and number of hours (see 'hours'). Also converts tripduration into hours instead of seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#seconds since start of week\n",
    "seconds = np.zeros(df.shape[0])\n",
    "dow = np.zeros(df.shape[0])\n",
    "for i,p in enumerate(df.iterrows()):\n",
    "    hiredatetime = datetime.strptime(p[1]['starttime'], '%m/%d/%Y %H:%M:%S')\n",
    "    midnight = hiredatetime.replace(hour=0, minute=0, second=0, microsecond=0)\n",
    "    dow[i] = hiredatetime.weekday()    \n",
    "    seconds[i] = (hiredatetime - midnight).seconds + dow[i]*(3600*24.0)\n",
    "df['seconds'] = seconds #total number of seconds\n",
    "df['hours'] = seconds/3600.0 #total number of hours\n",
    "df['dow'] = dow\n",
    "df['tripduration_hours'] = df['tripduration']/3600.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methods to analyse data\n",
    "\n",
    " - bin_dataframe: Given a dataframe and a list of dimensions to use, generate a histogram of data (and provide the bin boundaries etc).\n",
    " - buildlist: Produces a matrix. Each row describes one bin. Each pair of columns describes the bin edges in one dimension.\n",
    " - findcentres: Produces a matrix, in which each column is a bin centre.\n",
    " - get_Gaussian_DP_noise: Calculate the sigma (standard deviation) value for the specified DP parameters.\n",
    " - generate_Gaussian_DP_noise: Generates DP noises of the required scale and shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def buildlist(bins):\n",
    "    #Produces a matrix. Each row describes one bin.\n",
    "    #bins: a list of D numpy arrays (one for each dimension),\n",
    "    #each one specifying the boundaries of each bin (Mx bins)\n",
    "    #e.g. [np.array([1,2,3]),np.array([4,6,8,10])]\n",
    "    #the arrays contain varying number of bin boundaries,\n",
    "    #M1+1,M2+1..MD+1\n",
    "    #\n",
    "    #outputs: a D dimensional array containing the values\n",
    "    #\n",
    "    #returns\n",
    "    #a matrix of size (M1*M2*...*MD) x (2*D)\n",
    "    #each row is one bin in the D-dimensional histogram\n",
    "    #each pair of columns are the boundary values of a bin\n",
    "    #in one dimension, so in the example above the result\n",
    "    #would be:\n",
    "    # 1 2 4  6\n",
    "    # 1 2 6  8\n",
    "    # 1 2 8 10\n",
    "    # 2 3 4  6\n",
    "    # 2 3 6  8\n",
    "    # 2 3 8 10\n",
    "    #M1=2, M2=3, so the matrix size is (2*3) x (2*2) = 6x4\n",
    "    #\n",
    "    #This is the same order of items that feeding the histogram\n",
    "    #into squeeze produces.\n",
    "    boundaries = None\n",
    "    for b1,b2 in zip(bins[0][0:-1],bins[0][1:]):\n",
    "        if len(bins)>1:\n",
    "            new = np.array(buildlist(bins[1:]))\n",
    "            old = np.repeat(np.array([b1,b2])[None,:],len(new),axis=0)\n",
    "            \n",
    "            newrows = np.hstack([old,new])\n",
    "            \n",
    "        else:\n",
    "            newrows = np.array([b1,b2])[None,:]\n",
    "        if boundaries is None:\n",
    "            boundaries = newrows\n",
    "            \n",
    "        else:\n",
    "            boundaries = np.vstack([boundaries,newrows])\n",
    "\n",
    "    return boundaries\n",
    "\n",
    "def findcentres(area_row_list):\n",
    "    #takes the matrix (Lx(2D)) buildlist returns and\n",
    "    #finds the mean of each pair of bin boundaries\n",
    "    #reults in an LxD matrix\n",
    "    #\n",
    "    #for the example given in buildlist's comment,\n",
    "    #the output of this function would be:\n",
    "    # 1.5 5\n",
    "    # 1.5 7\n",
    "    # 1.5 9\n",
    "    # 2.5 5\n",
    "    # 2.5 7\n",
    "    # 2.5 9\n",
    "\n",
    "    out = []\n",
    "    for d in range(0,area_row_list.shape[1],2):\n",
    "        out.append(np.mean(area_row_list[:,d:(d+2)],axis=1))\n",
    "    return(np.array(out).T)\n",
    "\n",
    "def get_Gaussian_DP_noise(epsilon,delta,sensitivity):\n",
    "    csqr = 2*np.log(1.25/delta)\n",
    "    sigma = np.sqrt(csqr)*sensitivity/epsilon\n",
    "    return sigma\n",
    "\n",
    "def generate_Gaussian_DP_noise(sigma,shape):\n",
    "    noise = np.random.normal(0,sigma,shape)\n",
    "    return noise\n",
    "\n",
    "def bin_dataframe(df,axes,density=True,verbose=False):\n",
    "#Bins the data in a dataframe, by the list of tuples in 'axes'.\n",
    "#each tuple specifies the name of a column and the range and step size to bin it with, e.g.:\n",
    "#[('seconds',0,24*3600,60*5),('gender',None,None,1)] or leave out the range,\n",
    "#and just provide a single step size, and let the tool decide on the bounds\n",
    "#\n",
    "#If density is true, then the result is divided by the area of the bins, to give a\n",
    "#density. Useful if comparing between different bin sizes, etc.\n",
    "#\n",
    "#returns:\n",
    "#output = histogram\n",
    "#represent the data of the histogram in a list:\n",
    "#point_row_form = each row is one histogram cell's centroid location\n",
    "#area_row_form = each row is one histogram cell's location, with the bounds specified\n",
    "#output_row_form = each row is the value of that histogram cell\n",
    "#bins = list of arrays, each one a list of boundaries for each bin\n",
    "\n",
    "\n",
    "    bins = []\n",
    "    for i,axis in enumerate(axes):\n",
    "        column = axis[0]\n",
    "        start = axis[1]\n",
    "        end = axis[2]\n",
    "        step = axis[3]\n",
    "        if (start==None):\n",
    "            s = df[column]\n",
    "            s = s[~np.isnan(s)]\n",
    "            s = np.sort(s)\n",
    "            N = s.shape[0]\n",
    "            start = s[int(N*0.03)] #get rid of outliers\n",
    "            end = s[int(N*0.97)]\n",
    "            delta= (end-start)*0.04 #add 10% on each end, to catch any outside the range.\n",
    "            start -= delta\n",
    "            end += delta\n",
    "            #step = (end-start)/step            \n",
    "        axes[i] = column,start,end,step\n",
    "        bins.append(np.arange(start,end+step,step))\n",
    "    data = df[[axis[0] for axis in axes]].as_matrix()\n",
    "    \n",
    "    output = np.histogramdd(data,bins)[0]\n",
    "    area = np.prod([b[3] for b in axes])\n",
    "    output /= area\n",
    "    if verbose:\n",
    "        bincount = np.prod([len(b)-1 for b in bins])\n",
    "        print(\"Bin count: %d\" % bincount)\n",
    "        datacount =  (df.shape[0])\n",
    "        print(\"Data Length: %d\" % datacount)\n",
    "        print(\"Average occupancy: %0.2f\" % (1.0*datacount/bincount))\n",
    "        print(\"Area: %0.5f\" % area)\n",
    "        print(\"Area x Density: %0.4f\" % (np.sum(output)*area))\n",
    "        print(\"%0.2f%% of data were not included.\" % (100*(1-((np.sum(output)*area)/datacount))))\n",
    "    \n",
    "    area_row_form = buildlist(bins)\n",
    "    point_row_form = findcentres(area_row_form)\n",
    "    output_row_form = output.flatten() #TODO: CHECK ORDER IS CORRECT\n",
    "    #assert np.sum(output)==df.shape[0], \"Not all data points have been counted\"\n",
    "    return output,point_row_form,area_row_form,output_row_form,bins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cycle hire times\n",
    "\n",
    "To give you an idea of cyclist hire times, here's a histogram!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYcAAAEACAYAAABYq7oeAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAFsJJREFUeJzt3W+sZHdZwPHv0912oYWWFdOd2C29+Ie2IIJVSk0xTIC0\nBZOWF6YUFVsIvhAUggl2F17s8kapRgGDmKhYFxRq8V9rgtI2ZUSI5Y9tbaV/qNTdluK9QIAaNNm0\n9PHFnNmdvWfuvXNnzpw5Z+b7STZ75szMOc85M3Of8/x+v3NOZCaSJA07ad4BSJKax+QgSSoxOUiS\nSkwOkqQSk4MkqcTkIEkq2TI5RMSHI2ItIu4Zmvc7EXF/RNwdEX8TEacPPbc/Ih4qnr9kVoFLkmZn\nnMrheuDSdfNuAV6QmS8GHgL2A0TE84ErgfOBVwMfioioLlxJUh22TA6Z+VngO+vm3ZaZTxUP7wD2\nFtOXAzdk5pOZeZh+4riwunAlSXWoos/hTcAni+mzgEeHnnusmCdJapGpkkNEvBt4IjM/XlE8kqQG\n2DnpGyPiGuA1wCuGZj8GnD30eG8xb9T7vaiTJE0gM2felztu5RDFv/6DiMuAdwKXZ+bRodfdDFwV\nEadExHOBHwW+sNFCM7O1/w4cODD3GIx//nEsY/xtjn0R4q/LlpVDRHwM6ALPjohHgAPAu4BTgFuL\nwUh3ZOZbMvO+iLgRuA94AnhL1rk1kqRKbJkcMvMXRsy+fpPX/zbw29MEJUmaL8+QnlC32513CFMx\n/vlqc/xtjh3aH39dYl6tPhFhi5MkbVNEkA3qkJYkLRGTgySpxOQgSSoxOUiSSkwOkqQSk4MkqcTk\nIEkqMTlIkkpMDpKkEpODJKnE5CBJKjE5SJJKTA6SpBKTgySpxOQgSSoxOUiSSkwOkqQSk4MkqcTk\nIEkqMTlIkkpMDpKkEpODJKnE5CBJKjE5SJJKTA6SpBKTgySpZMvkEBEfjoi1iLhnaN7uiLglIh6M\niE9FxBlDz+2PiIci4v6IuGRWgUuSZmecyuF64NJ18/YBt2XmucDtwH6AiHg+cCVwPvBq4EMREdWF\nK0mqw5bJITM/C3xn3ewrgEPF9CHgtcX05cANmflkZh4GHgIurCZUSVJdJu1zODMz1wAycxU4s5h/\nFvDo0OseK+apgTqdFTqdlXmHIamBdla0nKxoOarR2tqReYcgqaEmTQ5rEbEnM9ciogN8o5j/GHD2\n0Ov2FvNGOnjw4LHpbrdLt9udMBxJWky9Xo9er1f7eiNz64P+iFgB/iEzX1g8vg74dmZeFxHXArsz\nc1/RIf2XwEvpNyfdCvxYjlhJRIyarRoNxgr4OUjtERFk5swH+mxZOUTEx4Au8OyIeAQ4ALwX+ERE\nvAk4Qn+EEpl5X0TcCNwHPAG8xQwgSe0zVuUwkxVbOcydlYPUPnVVDp4hLUkqMTlIkkpMDpKkEpOD\nJKnE5CBJKjE5SJJKTA6SpBKTgySpxOQgSSoxOUiSSkwOkqQSk4MkqcTkIEkqMTlIkkpMDjPU6awQ\nEd6nWVLreD+HGerfLyGBaOQ9E7yfg9Q+3s9BkjQ3JgdJUonJQa1g/41UL/scZsg+h+o0fV9KdbHP\nQZI0NyYHSVKJyWHp7bItX1KJfQ4z1PR28kGfQ5NjHGj6vpTqYp+DJGluTA6SpBKTgySpxOQgSSox\nOUiSSqZKDhHxjoj4j4i4JyL+MiJOiYjdEXFLRDwYEZ+KiDOqClaSVI+Jk0NE/BDw68AFmfkTwE7g\n9cA+4LbMPBe4HdhfRaCSpPpM26y0AzgtInYCTwceA64ADhXPHwJeO+U6JEk1mzg5ZObXgd8DHqGf\nFB7PzNuAPZm5VrxmFTizikAlSfXZOekbI+JZ9KuEc4DHgU9ExC/SP4112Iansx48ePDYdLfbpdvt\nThqOJC2kXq9Hr9erfb0TXz4jIn4euDQzf6V4/AbgIuAVQDcz1yKiA3w6M88f8X4vnzFnXj5Dap82\nXD7jEeCiiHha9H+5rwTuA24GrileczVw01QRruNNX9RUfje1SKa68F5EHACuAp4A7gLeDDwTuBE4\nGzgCXJmZ3x3x3okqhzYdQTY9ViuHarUhRrVfXZVD667K2qYfYNNjrTI5DI6WV1cPTxvWSE3fl9CO\nGNV+JoeN30dbfoBNj7XK5DDrW442fV9CO2JU+7Whz0EbGLQ9S1JbWTnMwHCMTY7VyqFabYhR7Wfl\nIEmaG5ODJKnE5CBJKjE5SJJKTA6SpBKTgySpxOQgSSoxOUiSSkwOkqQSk4MkqcTkIEkqMTlIkkpM\nDpKkEpODJKnE5CBJKjE5SJJKTA6SpBKTgySpZCmTw+Aez53OyrxDkaRGWsp7SM/6Xr/eQ7p6bbg/\ncxtiVPt5D2mpYlaM0visHKwcWJbKoc7PvYmftxaDlYMkaW5MDpKkEpNDg3U6K0vfPj7oJ5BUr6n6\nHCLiDOBPgR8HngLeBHwF+CvgHOAwcGVmPj7ivfY5jLWcWbfjQ5P7HKrsv7HPQYugLX0OHwA+mZnn\nAy8CHgD2Abdl5rnA7cD+KdchSarZxJVDRJwO3JWZP7Ju/gPAyzNzLSI6QC8zzxvxfiuHsZZj5WDl\nIB3XhsrhucC3IuL6iLgzIv44Ik4F9mTmGkBmrgJnVhGoJKk+O6d87wXAWzPzSxHxPvpNSusPmTY8\nhDp48OCx6W63S7fb3cbqdxER7NlzDqurh7fxPi2bTmeFtbUj8w5Dmkiv16PX69W+3mmalfYA/5qZ\nP1w8fhn95PAjQHeoWenTRZ/E+vdP3aw0aQlvs9KJy1/0ZqW6Tkq0WUl1aHyzUtF09GhEPK+Y9Urg\ny8DNwDXFvKuBm6YJUJJUv2mHsr6I/lDWk4GHgTcCO4AbgbOBI/SHsn53xHutHMZajpWDlYN0XF2V\nQ6uvrWRymE4VyWF9e77JweSg2Wp8s5IEFInBP4TSojE5SJJKTA6SpBKTgySpxOQgSSoxOUiSSkwO\nkqQSk4MkqcTkIEkqMTlIkkpMDpKkEpODJKnE5CBJKjE5SJJKTA6N178daqezMu9AxtC8WJsUi6rT\n6aw07ru2aLyfQwvu5zCrWKtY/onbOt2ytl7+9pe9fhu9n8NiWOZ97f0cJElzY3KQJJWYHKQ5G7Sf\n79hxmm3oagz7HOxzmGr59jmsX9ekMR7fh8vWhj4J+xzsc5AkzYHJQZJUYnKQJJWYHKRGad6JhFpO\nO+cdgKRhR4FkbW3m/Y3Spqwc6A8l9EhNkxoMRZUWiUNZM4/9sJsy/PLE5TDVMma9fIeyVhXj7Pbh\nInIoq0NZJUlzMHVyiIiTIuLOiLi5eLw7Im6JiAcj4lMRccb0YUqS6lRF5fB24L6hx/uA2zLzXOB2\nYH8F61DD2M6+GUccqf2mSg4RsRd4DfCnQ7OvAA4V04eA106zDjXT2toR+m2+KhuMODoy70CkiU1b\nObwPeCcn/pXYk5lrAJm5Cpw55TokSTWb+DyHiPg5YC0z746I7iYv3fDw8uDBg8emu90u3e5mi5Gq\nsotOZ4XV1cPzDkTaUq/Xo9fr1b7eiYeyRsRvAb8EPAk8HXgm8HfATwPdzFyLiA7w6cw8f8T7Hco6\n1nKYahmzWv6obZx0WdtdTxVDWWG2n/d2YnUo6/Y5lLXBQ1kz812Z+ZzM/GHgKuD2zHwD8A/ANcXL\nrgZumjrKmdllp6q0Ce/VvLxmcfmM9wI3RsSbgCPAlTNYR0X6HYfHj9gkDRsMPPByHstn6c+QrquZ\nwWalaWK0WWlemtp809S46tD4ZiVJ0uJauuRg26m0eOwbqd7SNSvNq5nBZqVpYmzf522z0myNHnHY\nvDhnwWYlSdLcmBwkSSUmhwbyonbSuDxXaVZMDg3kRe2kcQ3OVVLVTA6SpBKTwxKy2aoN5ndPCIeF\nChzKShOHstZ7X+bJlu9Q1s1jrGIo67yGZjZ5aO3AtPu8zRzKqnXae3exTmellXEvEz8jrWfl0KLK\noaojo7orh2kuid7Wz7ttlcPwZ2Tl0GxWDussQzv54mxje6ucOrXx825Tf0SbYm2i1lQOVR0pNLly\n2OpovE2VQ/nSBlYO472vHGuTKoemXKZinH3elFirZuWgytV/pNrkE5SsbqTNzOJmP2qo4yfX1fUH\nu8k3U+rH5k1spNGsHNQ4jpyZl80qvSZXgZoFKwc1Tr/CUf02q/SaXAVqFqwcJEklJgdJUonJQZqZ\nXfad1G7UPvdzmITnOSzReQ5VXg9p3Fgn2b8bj7kfHefgh7+6enjDZc3yOlCbfTcH2zH++8qx1nme\nw3Y+06ae5wCjz9dYlHMd6jrPwQ5ptZ4d2FL1bFbSgvCktkXj5S/my8pBC8KT2hbN4KRNP9P5aEXl\n4JHDMI+QVZ12XPzP7/w8tKJDeuPOsuXskJ60Q3BRO6TXf6ajLhduh/Q4sZfXPXpePR3Sm332dkh7\n4b3WWYajm8XaxnqOShev/bw8PHTxtnG5TVw5RMRe4CPAHuAp4E8y8w8iYjfwV8A5wGHgysx8fMT7\nF7Jy2Gj5i1Q5TDLsEZpbOcxmiPDGR7GbH/1WF9f2Yy+ve/S80Z9p1ZfItnIYrQ2Vw5PAb2TmC4Cf\nAd4aEecB+4DbMvNc4HZg//Rhavl4oTeNZmVSj4mTQ2auZubdxfT3gPuBvcAVwKHiZYeA104bpJbR\n4EJv0ok8r6UelfQ5RMQK8GLgDmBPZq5BP4EAZ1axDmk8/Ypjx47TGlt5bG+EkCN1NB9Tn+cQEc8A\n/hp4e2Z+LyLWH+55+Kca9SuOp55a327eHNu76ZLnb2g+pkoOEbGTfmL4aGbeVMxei4g9mbkWER3g\nGxu9/+DBg8emu90u3W53mnCm1D9C27PnnJHX6KlKp7PC2tqRma9Hmp/+SKbqv9/L2Q/V6/Xo9Xq1\nr3eq8xwi4iPAtzLzN4bmXQd8OzOvi4hrgd2ZuW/Ee2sdrTT4o3xctePet1p+Zg69ZrNRN/MdrTSc\nvIATEtl29mEVo5U2WtaePeecENdWrx8v1qcBRydK2pOMnNlerLMbtTSr0Urrn5s+tu3EtYt+xQVV\nxtMUdY1WmmYo68XAZ4B76X8CCbwL+AJwI3A2cIT+UNbvjnh/rclhOz+CSfbJVss//gdi4/U0ITls\ntB3j/YHbOFaoLjmMjmu819e3X00O80sOo7fD5LA9EzcrZebngB0bPP2qSZercVVdui9nyb6sylXg\nJOppitV8eOG91jpa8ZA+7xG8TLbXKb4RO8sX2UJePqPTWTk2lNEhgNKsjao6HYLbdgtZORw/ovao\nRpq9UVWnVUXbLWTl0DybH1lNe9nk2V/wrKn9EbOIa7z7DY+/z5u678axa25rtuKYv4W8ZPeoUUHz\nHq00zYiQ7YyE2Sz2SS6zXeV2zGK00rxincfnXeVvdfzvwmSxDj83+e9p+nU7WmlyVg5zN7+jMy2f\ndtzcR01gcpi7o1u/RKrI8VFK0uZMDtqAFc1suF/VDiYHbcCKZjbcr2oHk0OJ47O1HfVVAt6GU3Va\niORQ7Y9mMD67yrOPZ6XNwyQn1bRmmfoqgUF/QTu+m2q7hTgJ7viPZtn+UC7jJS9slpHqsBCVgySp\nWiYHSVKJyUFqhV0nTG+nj23QJ7djx2kziawdHGiyXQvR5yAtvqPrpsfvYxv0yfXvq72svBDgdi1A\n5bDV6JVlHNFT5hGTpO1YgOSw1eiVwYie5ebwR0nbsQDJYb68kFl13JfyO9AcJocpeSGz6rgv5Xeg\nORYsOdi/IGkzjloa14KNVlrGM4Yljc9RS+NasMqhSh5hqMw28e3yd9RWc00OEcH73//BeYawiTZd\ngE91sU18u/wdtdWcK4ff5eGHD883hC2Nd8N5jcfLTldpFzt2nDZyf3Y6Kw3bx/6O2sZmpS0dbdFR\nT/N/gMevoLva+Fib7yhPPfV/DPbncJJYWzvSsO/t0WMxbpTQ1CwL1iG97NqUyNoUaxu0oaO1H2P/\nMh5Nj1VWDqqJw4zr1559bkd/88wsOUTEZRHxQER8JSKundV61BZexqR+7dnndvQ3z0ySQ0ScBHwQ\nuBR4AfD6iDhv3PcPX2K4qUcTvV5v3iGMZeMjsl7doVSsN8Nlnzj8cjaXvO5VuKxhdVQLvRkvf9Z6\n8w6gFWZVOVwIPJSZRzLzCeAG4Ipx33z8EsP9zrYmakty2PiIrFdzJFXrzXDZJw6/PPH7WJVehcsa\nVke10Jvx8metN+8AWmFWyeEs4NGhx18r5rVU+UjyPe95T6NHW7SjDXdXwyvE9rTZt9OuGVRkqspc\nO6Sf9rQ/Y9euk+cZwphGHUkeaPRom3a04Q5GrzS1QmxPm307DX/+aprIrP7LHxEXAQcz87Li8T4g\nM/O6odf4q5OkCWTmzEvaWSWHHcCDwCuB/wa+ALw+M++vfGWSpMrN5CS4zPx+RPwacAv9pqsPmxgk\nqT1mUjlIktptLh3S8zxBLiI+HBFrEXHP0LzdEXFLRDwYEZ+KiDOGntsfEQ9FxP0RccnQ/Asi4p5i\nG94/NP+UiLiheM+/RsRzhp67unj9gxHxyxPGvzcibo+IL0fEvRHxtrZsQ0TsiojPR8RdRewH2hL7\nuu04KSLujIib2xZ/RByOiH8vPoMvtDD+MyLiE0U8X46Il7Yl/oh4XrHf7yz+fzwi3tbY+DOz1n/0\nE9J/AucAJwN3A+fVuP6XAS8G7hmadx3wm8X0tcB7i+nnA3fRb35bKeIeVFufB15STH8SuLSY/lXg\nQ8X064AbiundwFeBM4BnDaYniL8DvLiYfgb9vp3z2rINwKnF/zuAO+ifE9OK2Ie24R3AXwA3t/D7\n8zCwe928NsX/58Abi+mdxfJaE//QdpwEfB04u6nx15YUhnbKRcA/Dj3eB1xbcwzncGJyeADYU0x3\ngAdGxQb8I/DS4jX3Dc2/CvijYvqfgJcW0zuAb6x/TfH4j4DXVbAtfw+8qm3bAJwKfAl4SZtiB/YC\ntwJdjieHNsX/X8Cz181rRfzA6cBXR8xvRfzrYr4E+Jcmxz+PZqUmniB3ZmauAWTmKnBmMX99rI8V\n886iH/fA8DYce09mfh94PCJ+YJNlTSwiVuhXQXfQ/3I1fhuKJpm7gFXg1sz8YltiL7wPeCcnngDR\npvgTuDUivhgRb25Z/M8FvhUR1xdNM38cEae2KP5hrwM+Vkw3Mn6vyjpabv2Ssc1kPHJEPAP4a+Dt\nmfk9yjE3chsy86nM/En6R+AXRsQLaEnsEfFzwFpm3r3FchsZf+HizLwAeA3w1oj4WVqy/+k3r1wA\n/GGxDf9L/+i6LfH3FxhxMnA58IliViPjn0dyeAx4ztDjvcW8eVqLiD0AEdEBvlHMf4x+m+DAINaN\n5p/wnuif73F6Zn6bCrc7InbSTwwfzcyb2rgNmfk/9C9yc1mLYr8YuDwiHgY+DrwiIj4KrLYkfjLz\nv4v/v0m/SfJC2rP/vwY8mplfKh7/Df1k0Zb4B14N/Ftmfqt43Mz4J20zm6KtbQfHO6RPod8hfX7N\nMawA9w49vo6ibY/RHUKn0C9phzuEBp2pQb9D6LJi/ls43iF0FaM7hAbTz5ow/o8Av79uXuO3AfhB\nik4w4OnAZ+gfwTY+9hHb8nKO9zn8Thvip9/P84xi+jTgc/Tbvluz/4F/Bp5XTB8oYm9N/MWyPg5c\n3fTfbm1/kNftnMvoj7J5CNhX87o/Rn+UwFHgEeCNxc66rYjpluGdBuwvPpT7gUuG5v8UcG+xDR8Y\nmr8LuLGYfwewMvTcNcX8rwC/PGH8FwPfp59U7wLuLPbnDzR9G4AXFvHeDdwDvLuY3/jYR2zLcHJo\nRfz0/8AMvjf3Uvz22hJ/sYwXAV8stuNv6f+xa1P8pwLfBJ45NK+R8XsSnCSpxA5pSVKJyUGSVGJy\nkCSVmBwkSSUmB0lSiclBklRicpAklZgcJEkl/w+aqHZYDvIFcwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fb6173d76d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "plt.hist(seconds,range(0,24*60*60*7,60*60*1));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyse the data\n",
    "\n",
    "Here we specify the boundary list, and then produce a series of variables. We produce two similar sets, one is a 'lower resolution' histogram, containing larger bins. This is used to train the GPs. The second set has a higher resolution and is used to test the predictions.\n",
    "\n",
    "**output** is the binned data, while **point_row_form**, **area_row_form**, **output_row_form**, **bins** are other variables describing the shape of the binning. \n",
    "\n",
    "**output_test** and associated variables are from a similar analysis, but with smaller bin sizes.\n",
    "\n",
    "We train on the low-res data, and then test how well it predicts the hi-res data. We do this with the assumption that, in future, people will want to estimate the hi-res data using low-res binned data (e.g. from a differentially private treatment of a dataset)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit the two GP regression models\n",
    "\n",
    "Here we fit two GP regression models to the low-res binned data. One uses the 'rbf' kernel, while the other uses the 'integral' kernel.\n",
    "\n",
    "### Fit the integral model (held in 'm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nan"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.NaN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def transform(data,normalisation_mean=np.NAN,normalisation_std=np.NAN):\n",
    "    data[data<0] = 0\n",
    "    result = np.sqrt(data)\n",
    "    if normalisation_mean is np.NAN:\n",
    "        normalisation_mean = np.mean(result)\n",
    "        normalisation_std = np.std(result)\n",
    "    result -= normalisation_mean\n",
    "    result /= normalisation_std\n",
    "    return result, normalisation_mean, normalisation_std\n",
    "\n",
    "def untransform(data,normalisation_mean,normalisation_std):\n",
    "    data *= normalisation_std\n",
    "    data += normalisation_mean\n",
    "    result = data**2\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import GPy\n",
    "\n",
    "def fitmodels(output,point_row_form,area_row_form,output_row_form,bins,output_test,point_row_form_test,area_row_form_test,output_row_form_test,bins_test):\n",
    "    #add the DP noise, and fit the integral kernel and RBF kernel models.\n",
    "   \n",
    "    #add the noise and normalise\n",
    "    sigma = get_Gaussian_DP_noise(0.2,0.01,1.0)\n",
    "    noisy_output = output_row_form + generate_Gaussian_DP_noise(sigma,len(output_row_form))\n",
    "    noisy_output, normalisation_mean, normalisation_std = transform(noisy_output)\n",
    "\n",
    "\n",
    "    #initialise the INTEGRAL kernel\n",
    "    init_length_scales = 0.02*np.array([np.max(b)-np.min(b) for b in bins])\n",
    "    kernel = GPy.kern.Multidimensional_Integral_Limits(input_dim=area_row_form.shape[1], variances=5.0, lengthscale=init_length_scales)\n",
    "    m = GPy.models.GPRegression(1.0*area_row_form,1.0*noisy_output[:,None],kernel)#,normalizer=True)\n",
    "    m.integral.variances = 1.0\n",
    "    m.Gaussian_noise.variance = 0.01\n",
    "\n",
    "    #OPTIMISE the integral kernel\n",
    "    m.optimize_restarts(num_restarts=1)#messages=True)\n",
    "\n",
    "\n",
    "    #initialise the RBF Kernel\n",
    "    rbfkernel = GPy.kern.RBF(input_dim=point_row_form.shape[1], lengthscale=init_length_scales, ARD=True)\n",
    "    m_rbf = GPy.models.GPRegression(1.0*point_row_form,1.0*noisy_output[:,None],rbfkernel)\n",
    "    m_rbf.rbf.variance = 100.0\n",
    "    m_rbf.rbf.lengthscale=[4.49,0.295] #TODO!! WHY DO WE HAVE TO SET THE LENGTHSCALE FOR THE RBF KERNEL!?!?!?!\n",
    "    #m_rbf.rbf.lengthscale.constrain_bounded(0.1,10)\n",
    "\n",
    "    #Optimise the RBF kernel\n",
    "    m_rbf.optimize_restarts(num_restarts=1)#messages=True)\n",
    "\n",
    "    return m,m_rbf,normalisation_mean,normalisation_std\n",
    "\n",
    "\n",
    "#Get the distribution of histogram bins\n",
    "boundarylist = [('hours',0,24*7,3), ('tripduration_hours',0,1,20.0/60.0)]\n",
    "test_boundarylist = [('hours',0,24*7,1), ('tripduration_hours',0,1,10.0/60.0)]\n",
    "#get the actual histogram results\n",
    "output,point_row_form,area_row_form,output_row_form,bins = bin_dataframe(df,boundarylist)\n",
    "output_test,point_row_form_test,area_row_form_test,output_row_form_test,bins_test = bin_dataframe(df,test_boundarylist)\n",
    "m,m_rbf,normalisation_mean,normalisation_std = fitmodels(output,point_row_form,area_row_form,output_row_form,bins,output_test,point_row_form_test,area_row_form_test,output_row_form_test,bins_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assessment\n",
    "\n",
    "### Integral kernel\n",
    "\n",
    "First we plot the 'integral' kernel's fit. We plot it for short rides and long rides. Notice, short rides are more popular in the week, while long hires are more common at the weekend & longer ones are less popular in general."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "m.plot(fixed_inputs=[(1,0),(2,5.0/60),(3,0)],plot_data=False)\n",
    "sel = point_row_form_test[:,1]==5.0/60\n",
    "transformed_data,_,_ = transform(output_row_form_test[sel],normalisation_mean,normalisation_std)\n",
    "plt.plot(point_row_form_test[sel,0],transformed_data,'x')\n",
    "plt.title('5 minute hires')\n",
    "\n",
    "m.plot(fixed_inputs=[(1,0),(2,45.0/60),(3,0)],plot_data=False)\n",
    "sel = point_row_form_test[:,1]==45.0/60\n",
    "plt.plot(point_row_form_test[sel,0],transformed_data,'x')\n",
    "plt.title('45 minute hires')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RBF Kernel\n",
    "\n",
    "Similarly we look at the results of using the rbf kernel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "m_rbf.plot(fixed_inputs=[(1,5.0/60)],plot_data=False)\n",
    "sel = point_row_form_test[:,1]==5.0/60\n",
    "plt.plot(point_row_form_test[sel,0],transformed_data,'x')\n",
    "\n",
    "m_rbf.plot(fixed_inputs=[(1,45.0/60)],plot_data=False)\n",
    "sel = point_row_form_test[:,1]==45.0/60\n",
    "plt.plot(point_row_form_test[sel,0],transformed_data,'x')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RMSE\n",
    "\n",
    "Here we calculate the RMSE of the predictions of the two models, compared to the high-res 'test' histogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#area_row_form_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "int_rmses = []\n",
    "rbf_rmses = []\n",
    "Nits = 10\n",
    "## Find the sum squared error for the two methods\n",
    "for it in range(Nits):\n",
    "    print it\n",
    "    m,m_rbf,normalisation_mean,normalisation_std = fitmodels(output,point_row_form,area_row_form,output_row_form,bins,output_test,point_row_form_test,area_row_form_test,output_row_form_test,bins_test) #generate another DP sample and refit\n",
    "    int_means = []\n",
    "    rbf_means = []\n",
    "    for point_row,area_row in zip(point_row_form_test,area_row_form_test):\n",
    "        [int_mean,var] = m.predict_noiseless(np.array([area_row]))\n",
    "        int_means.append(untransform(int_mean[0][0],normalisation_mean,normalisation_std))\n",
    "\n",
    "        [rbf_mean,var] = m_rbf.predict_noiseless(np.array([point_row]))\n",
    "        rbf_means.append(untransform(rbf_mean[0][0],normalisation_mean,normalisation_std))\n",
    "        \n",
    "    int_rmses.append(np.sqrt(mean_squared_error(int_means,output_row_form_test)))\n",
    "    rbf_rmses.append(np.sqrt(mean_squared_error(rbf_means,output_row_form_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"Integral Kernel RMSE: %6.2f (%0.2f)\" % (np.mean(int_rmses),np.std(int_rmses)/np.sqrt(Nits)))\n",
    "print(\"     RBF Kernel RMSE: %6.2f (%0.2f)\" % (np.mean(rbf_rmses),np.std(rbf_rmses)/np.sqrt(Nits)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems out integral kernel is doing a much better job of fitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Noise on the outputs (GP sensitivity)\n",
    "\n",
    "In this section we look at the results of applying DP directly to the GP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def k(x,xprime,l):\n",
    "    #kernel function: takes two values x and xprime, and finds the covariance between them.\n",
    "    #x, xprime = vectors each representing one point each (so Dx1)\n",
    "    #l = lengthscale vector (Dx1).\n",
    "    #returns a scalar value describing the covariance between these locations.\n",
    "    return np.exp(-.5*np.sum(((x-xprime)/l)**2))\n",
    "    #return m.kern.K(np.array([[x]]),np.array([[xprime]])) #could try using GPy kernels in future?\n",
    "\n",
    "def inf_norm(A):\n",
    "    #returns the infinity norm of A.\n",
    "    return np.max(np.sum(np.abs(A),1))\n",
    "\n",
    "def get_noise_scale(y_in,test_inputs,training_inputs,pseudo_inputs,lengthscales,sigma,verbose=False):\n",
    "    #Finds the infinity norm of the inverse covariance matrix\n",
    "    #and the infinity norm of the $Q^{-1} K_{uf} (\\Lambda + \\sigma^2 I)^{-1}$\n",
    "    #matrix (which relies on inducing inputs to reduce the value\n",
    "    #of this norm) See the Differentially Private Gaussian Processes\n",
    "    #paper and [1] for more details.\n",
    "    #\n",
    "    #This assumes we're using the RBF kernel. It also uses the lengthscales\n",
    "    #specified.\n",
    "    #\n",
    "    #[1] Hall, Rinaldo and Wasserman. DP for functions and functional data. \n",
    "    #Journal of Machine Learning Research 14.Feb (2013): 703-727.\n",
    "    #\n",
    "    #parameters:\n",
    "    #y = the output values (currently only handles single output functions)\n",
    "    #test_inputs = the input values to test with\n",
    "    #training_inputs = the training data (describes the GP)\n",
    "    #pseudo_inputs = the inducing inputs\n",
    "    #lengthscales = a vector of lengthscales (one for each input dimension)\n",
    "    #sigma = noise standard deviation.\n",
    "    #verbose (optional) = set to true for verbiage.\n",
    "    #\n",
    "    #returns:\n",
    "    # test_cov = the covariance matrix (based on the kernel and the\n",
    "    #            locations of the test inputs.\n",
    "    #\n",
    "    # normal_inf_norm = the infinity norm of the inverse covariance matrix\n",
    "    # pseudo_inf_norm = the equivalent when using inducing inputs\n",
    "\n",
    "    #normalise outputs (y): zero mean, unit variance\n",
    "    y = np.sqrt(y_in) #make it a bit more normal \n",
    "    \n",
    "    ymean = np.mean(y)\n",
    "    ystd = np.std(y)\n",
    "    y = y - ymean\n",
    "    y = y / ystd\n",
    "    sigma = sigma / ystd\n",
    "\n",
    "        \n",
    "    sigmasqr = sigma**2\n",
    "\n",
    "    #covariance between test inputs\n",
    "    print \"Calculating covariance between test inputs\"\n",
    "    sys.stdout.flush()\n",
    "    test_cov = np.zeros([len(test_inputs),len(test_inputs)])\n",
    "    for i,t_in1 in enumerate(test_inputs):\n",
    "        for j,t_in2 in enumerate(test_inputs):\n",
    "            test_cov[i,j] = k(t_in1,t_in2,lengthscales)\n",
    "\n",
    "    #covariance between training inputs and test inputs\n",
    "    print \"Calculating covariance between training inputs and test inputs\"\n",
    "    sys.stdout.flush()\n",
    "    K_Nstar = np.zeros([len(training_inputs),len(test_inputs)])\n",
    "    for i,t_in in enumerate(training_inputs):\n",
    "        for j,p_in in enumerate(test_inputs):\n",
    "            K_Nstar[i,j] = k(t_in,p_in,lengthscales)\n",
    "\n",
    "    #covariance between training inputs and pseudo inputs\n",
    "    print \"Calculating K_NM\"\n",
    "    sys.stdout.flush()\n",
    "    K_NM = np.zeros([len(training_inputs),len(pseudo_inputs)])\n",
    "    for i,t_in in enumerate(training_inputs):\n",
    "        for j,p_in in enumerate(pseudo_inputs):\n",
    "            K_NM[i,j] = k(t_in,p_in,lengthscales)\n",
    "\n",
    "    #covariance between pseudo inputs\n",
    "    print \"Calculating K_MM\"\n",
    "    sys.stdout.flush()\n",
    "    K_MM = np.zeros([len(pseudo_inputs),len(pseudo_inputs)])\n",
    "    for i,p_in1 in enumerate(pseudo_inputs):\n",
    "        for j,p_in2 in enumerate(pseudo_inputs):\n",
    "            K_MM[i,j] = k(p_in1,p_in2,lengthscales)\n",
    "    #K_MM += np.eye(len(pseudo_inputs))*10.0\n",
    "    invK_MM = np.linalg.inv(K_MM)\n",
    "\n",
    "    #covariance between training inputs\n",
    "    print \"Calculating K_NN\"\n",
    "    sys.stdout.flush()\n",
    "    K_NN = np.zeros([len(training_inputs),len(training_inputs)])\n",
    "    for i,t_in1 in enumerate(training_inputs):\n",
    "        for j,t_in2 in enumerate(training_inputs):\n",
    "            K_NN[i,j] = k(t_in1,t_in2,lengthscales)\n",
    "    #K_NN += np.eye(len(training_inputs))*10.0\n",
    "\n",
    "\n",
    "    #covariance between test inputs and pseudo inputs\n",
    "    print \"Calculating K_star\"\n",
    "    sys.stdout.flush()\n",
    "    K_star = np.zeros([len(test_inputs),len(pseudo_inputs)])\n",
    "    for i,t_in in enumerate(test_inputs):\n",
    "        for j,p_in in enumerate(pseudo_inputs):\n",
    "            K_star[i,j] = k(t_in,p_in,lengthscales)\n",
    "\n",
    "    #lambda values are the diagonal of the training input covariances minus \n",
    "    #(cov of training+pseudo).(inv cov of pseudo).(transpose of cov of training+pseudo)\n",
    "    print \"Calculating lambda\"\n",
    "    sys.stdout.flush()\n",
    "    lamb = np.zeros(len(training_inputs))\n",
    "    for i,t_in in enumerate(training_inputs):\n",
    "        lamb[i] = K_NN[i,i] - np.dot(np.dot(K_NM[i,:].T,invK_MM),K_NM[i,:])\n",
    "\n",
    "    #this finds (\\Lambda + \\sigma^2 I)^{-1}\n",
    "    invlambplussigma = np.diag(1.0/(lamb + sigmasqr)) #note diagonal so can do the inversion first\n",
    "    Q = K_MM + np.dot(np.dot(K_NM.T, invlambplussigma),K_NM)\n",
    "\n",
    "    if (verbose):\n",
    "        print K_star.shape\n",
    "        print Q.shape\n",
    "        print K_NM.shape\n",
    "        print invlambplussigma.shape\n",
    "        print y.shape\n",
    "\n",
    "    #find the mean at each test point\n",
    "    psuedo_mu = np.dot(np.dot(np.dot(np.dot(K_star, np.linalg.inv(Q)),K_NM.T),invlambplussigma),y)\n",
    "    normal_mu = np.dot(np.dot(K_Nstar.T,np.linalg.inv(K_NN+sigmasqr*np.eye(K_NN.shape[0]))),y)\n",
    "\n",
    "    #un-normalise our estimates of the mean (one using the pseudo inputs, and one using normal GP regression)\n",
    "    psuedo_mu = psuedo_mu * ystd\n",
    "    psuedo_mu = psuedo_mu + ymean\n",
    "    normal_mu = normal_mu * ystd\n",
    "    normal_mu = normal_mu + ymean\n",
    "    y = y * ystd\n",
    "    y = y + ymean\n",
    "    \n",
    "    psuedo_mu=psuedo_mu**2\n",
    "    normal_mu=normal_mu**2\n",
    "    y = y**2\n",
    "    \n",
    "    #find the covariance for the two methods (pseudo and normal)\n",
    "    K_pseudo = np.dot(np.dot(np.linalg.inv(Q),K_NM.T),invlambplussigma)\n",
    "    pseudo_inf_norm = inf_norm(K_pseudo)\n",
    "    K_normal = K_NN + sigma * np.eye(K_NN.shape[0])\n",
    "    normal_inf_norm = inf_norm(np.linalg.inv(K_normal))\n",
    "\n",
    "    if verbose:\n",
    "        print \"Pseudo Inputs\"\n",
    "        print \"The matrix to apply infinity norm\"\n",
    "        print K_pseudo.shape\n",
    "        print \"has infinity norm\"\n",
    "        print pseudo_inf_norm #max sum of a ROW\n",
    "        print \" \"\n",
    "        print \"Normal Inputs\"\n",
    "        print \"The matrix to apply infinity norm\"\n",
    "        print K_normal.shape\n",
    "        print \"has infinity norm\"\n",
    "        print normal_inf_norm\n",
    "        \n",
    "    return test_cov, normal_inf_norm, pseudo_inf_norm, normal_mu, psuedo_mu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_boundarylist = [('hours',0,24*7,0.5), ('tripduration_hours',0,1,5.0/60.0)]\n",
    "test_boundarylist = [('hours',0,24*7,4.0), ('tripduration_hours',0,1,10.0/60.0)]\n",
    "pseudo_boundarylist = [('hours',0,24*7,3.0), ('tripduration_hours',0,1,15.0/60.0)]\n",
    "output_train,point_row_form_train,area_row_form_train,output_row_form_train,bins = bin_dataframe(df,train_boundarylist)\n",
    "output_test,point_row_form_test,area_row_form_test,output_row_form_test,bins_test = bin_dataframe(df,test_boundarylist)\n",
    "output_pseudo,point_row_form_pseudo,area_row_form_pseudo,output_row_form_pseudo,bins_pseudo = bin_dataframe(df,pseudo_boundarylist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(output_row_form_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ysens = 1.0\n",
    "eps = 0.2\n",
    "delta = 0.01\n",
    "\n",
    "\n",
    "test_inputs = point_row_form_test\n",
    "training_inputs = point_row_form_train\n",
    "pseudo_inputs = point_row_form_pseudo\n",
    "lengthscales = [1.0,5.0/60.0] #length scales of 1 hour and 5 minutes\n",
    "sigma = get_Gaussian_DP_noise(eps,delta,ysens)\n",
    "print \"Starting to get norms...\\n\"\n",
    "import sys \n",
    "sys.stdout.flush()\n",
    "y = output_row_form_train[:,None]\n",
    "test_cov, normal_inf_norm, pseudo_inf_norm, normal_mu, pseudo_mu = get_noise_scale(y,test_inputs,training_inputs,pseudo_inputs,lengthscales,sigma,verbose=True)\n",
    "print \"Done\"\n",
    "sys.stdout.flush()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dp_normal_rmse = []\n",
    "dp_pseudo_rmse = []\n",
    "its = 1000\n",
    "for sample in range(its):\n",
    "    G = np.random.multivariate_normal(np.zeros(len(test_inputs)),test_cov)\n",
    "    dp_normal_mu = np.array(normal_mu) + (G*ysens*np.sqrt(2*np.log(2/delta))*normal_inf_norm/eps)[:,None]\n",
    "    dp_pseudo_mu = np.array(pseudo_mu) + (G*ysens*np.sqrt(2*np.log(2/delta))*pseudo_inf_norm/eps)[:,None]\n",
    "    \n",
    "    dp_normal_rmse.append(np.sqrt(mean_squared_error(dp_normal_mu,output_row_form_test)))\n",
    "    dp_pseudo_rmse.append(np.sqrt(mean_squared_error(dp_pseudo_mu,output_row_form_test)))\n",
    "    \n",
    "print(\"Normal GP,   RMSE %0.3f\" % np.sqrt(mean_squared_error(normal_mu,output_row_form_test)))\n",
    "print(\"Pseudo GP,   RMSE %0.3f\" % np.sqrt(mean_squared_error(pseudo_mu,output_row_form_test)))\n",
    "print(\"Normal GP DP,RMSE %0.3f (%0.3f)\" % (np.mean(dp_normal_rmse),np.std(dp_normal_rmse)/np.sqrt(its)))\n",
    "print(\"Pseudo GP DP,RMSE %0.3f (%0.3f)\" % (np.mean(dp_pseudo_rmse),np.std(dp_pseudo_rmse)/np.sqrt(its)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=[14,4])\n",
    "sel = test_inputs[:,1]==0.25\n",
    "plt.plot(test_inputs[sel,0],normal_mu[sel],'-x')\n",
    "plt.plot(test_inputs[sel,0],output_row_form_test[sel],'-x')\n",
    "plt.title('Normal')\n",
    "sel = (training_inputs[:,1]>0.208) & (training_inputs[:,1]<0.209)\n",
    "plt.plot(training_inputs[sel,0],output_row_form_train[sel],'.',markersize=1)\n",
    "plt.ylim([0,200])\n",
    "\n",
    "\n",
    "plt.figure(figsize=[14,4])\n",
    "sel = test_inputs[:,1]==0.25\n",
    "plt.plot(test_inputs[sel,0],dp_normal_mu[sel],'-x')\n",
    "plt.plot(test_inputs[sel,0],output_row_form_test[sel],'-x')\n",
    "plt.title('Normal with DP added')\n",
    "sel = (training_inputs[:,1]>0.208) & (training_inputs[:,1]<0.209)\n",
    "plt.plot(training_inputs[sel,0],output_row_form_train[sel],'.',markersize=1)\n",
    "plt.ylim([0,200])\n",
    "\n",
    "plt.figure(figsize=[14,4])\n",
    "sel = test_inputs[:,1]==0.25\n",
    "plt.plot(test_inputs[sel,0],pseudo_mu[sel],'-x')\n",
    "plt.plot(test_inputs[sel,0],output_row_form_test[sel],'-x')\n",
    "plt.title('Pseudo')\n",
    "sel = pseudo_inputs[:,1]==0.125\n",
    "plt.vlines(pseudo_inputs[sel,0],np.zeros_like(pseudo_inputs[sel,0]),200*np.ones_like(pseudo_inputs[sel,0]),alpha=0.1)\n",
    "plt.ylim([0,200])\n",
    "sel = (training_inputs[:,1]>0.208) & (training_inputs[:,1]<0.209)\n",
    "plt.plot(training_inputs[sel,0],output_row_form_train[sel],'.',markersize=1)\n",
    "\n",
    "\n",
    "plt.figure(figsize=[14,4])\n",
    "sel = test_inputs[:,1]==0.25\n",
    "plt.plot(test_inputs[sel,0],dp_pseudo_mu[sel],'-x')\n",
    "plt.plot(test_inputs[sel,0],output_row_form_test[sel],'-x')\n",
    "plt.title('Pseudo with DP added')\n",
    "sel = (training_inputs[:,1]>0.208) & (training_inputs[:,1]<0.209)\n",
    "plt.plot(training_inputs[sel,0],output_row_form_train[sel],'.',markersize=1)\n",
    "plt.ylim([0,200])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above figures, blue are the predictions from the two models, while green(?) is the actual value. The vertical lines in the pseudo figure are the locations of the inducing inputs.\n",
    "\n",
    "Remember that the model didn't have access to the actual values (in blue) but instead had access to training data (marked with points). **The slice taken through the data is also slightly different, so these training data are plotted merely to give an indication of the sort of structure being learnt**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
