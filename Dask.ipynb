{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n",
    "\n",
    "## Get AWS setup\n",
    "\n",
    "1. From https://boto3.readthedocs.io/en/latest/guide/quickstart.html (Boto is the Amazon Web Services (AWS) SDK for Python)\n",
    "\n",
    "        sudo apt-get install awscli\n",
    "        pip install boto3\n",
    "2. Visit AWS -> IAM -> Add user -> Security Credentials -> Create Access Key\n",
    "3. Run `aws configure` and enter the ID, code, region (eu-west-1) - ireland, outputformat (blank - leave as JSON)\n",
    "4. Test with:\n",
    "\n",
    "        import boto3\n",
    "        s3 = boto3.resource('s3')\n",
    "        for b in s3.buckets.all():\n",
    "            print(b.name)\n",
    "        \n",
    "5. From http://distributed.readthedocs.io/en/latest/ec2.html,\n",
    "\n",
    "        pip install dask-ec2\n",
    "\n",
    "6. Visit AWS->EC2->Key pairs->Create key pair. I called mine \"research\". Save the keyfile in .ssh, chmod 600.\n",
    "7. Get the AMI we want to use (e.g. ubuntu 14.04). Check https://cloud-images.ubuntu.com/locator/ec2/ and search for e.g. `14.04 LTS eu-west-1 hvm ebs`. \n",
    "\n",
    "## Running DASK\n",
    "\n",
    "1. Run `dask-ec2 up --keyname YOUR-AWS-KEY --keypair ~/.ssh/YOUR-AWS-SSH-KEY.pem`. I found I had to also specify the region-name, the ami and tags as the first two have wrong defaults and the tool seems to fail if tags isn't set either.\n",
    "Also found using ubuntu 16.04 had a SSL wrong version number error (see https://github.com/dask/dask-ec2/issues/38 ).\n",
    "E.g.\n",
    "\n",
    "        dask-ec2 up --keyname research --keypair .ssh/research.pem --region-name eu-west-1 --ami ami-d37961b5 --tags research:dp\n",
    "\n",
    "Or less computation (2x2 = \\$0.22/hour):\n",
    "\n",
    "        dask-ec2 up --keyname research --keypair .ssh/research.pem --region-name eu-west-1 --ami ami-d37961b5 --tags research:dp --count 2 --volume-size 30 --type m4.large\n",
    "        \n",
    "Or greedy (8x36 = \\$14.5/hour):\n",
    "\n",
    "        dask-ec2 up --keyname research --keypair .ssh/research.pem --region-name eu-west-1 --ami ami-d37961b5 --tags research:dp --count 8 --volume-size 30 --type c4.8xlarge\n",
    "        \n",
    "Eventually after a long time, this will finish with:\n",
    "\n",
    "        Dask.Distributed Installation succeeded\n",
    "\n",
    "        Addresses\n",
    "        ---------\n",
    "        Web Interface:    http://54.246.253.159:8787/status\n",
    "        TCP Interface:           54.246.253.159:8786\n",
    "\n",
    "        To connect from the cluster\n",
    "        ---------------------------\n",
    "\n",
    "        dask-ec2 ssh  # ssh into head node\n",
    "        ipython  # start ipython shell\n",
    "\n",
    "        from dask.distributed import Client, progress\n",
    "        c = Client('127.0.0.1:8786')  # Connect to scheduler running on the head node\n",
    "\n",
    "        To connect locally\n",
    "        ------------------\n",
    "\n",
    "        Note: this requires you to have identical environments on your local machine and cluster.\n",
    "\n",
    "        ipython  # start ipython shell\n",
    "\n",
    "        from dask.distributed import Client, progress\n",
    "        e = Client('54.246.253.159:8786')  # Connect to scheduler running on the head node\n",
    "\n",
    "        To destroy\n",
    "        ----------\n",
    "\n",
    "        dask-ec2 destroy\n",
    "        Installing Jupyter notebook on the head node\n",
    "        DEBUG: Uploading file /tmp/tmp1GOH7d to /tmp/.__tmp_copy\n",
    "        DEBUG: Running command sudo -S bash -c 'cp -rf /tmp/.__tmp_copy /srv/pillar/jupyter.sls' on '54.246.253.159'\n",
    "        DEBUG: Running command sudo -S bash -c 'rm -rf /tmp/.__tmp_copy' on '54.246.253.159'\n",
    "        +---------+----------------------+-----------------+\n",
    "        | Node ID | # Successful actions | # Failed action |\n",
    "        +=========+======================+=================+\n",
    "        | node-0  | 17                   | 0               |\n",
    "        +---------+----------------------+-----------------+\n",
    "        Jupyter notebook available at http://54.246.253.159:8888/ \n",
    "        Login with password: jupyter\n",
    "\n",
    "#### Finding modules is a problem\n",
    "\n",
    "I found these not to work out the box. Critically, it failed with \"`distributed.utils - ERROR - No module named dask_searchcv.methods`\". I found I had to intstall the module on each worker:\n",
    "\n",
    "\n",
    "        local$ dask-ec2 ssh 1\n",
    "    dask1$ conda install dask-searchcv -c conda-forge -y\n",
    "\n",
    "I guess if I use GPy, a similar process is needed.\n",
    "\n",
    "Solutions:\n",
    "1) Make an image with the correct stuff already - bit awkward if you want to add a module later...?\n",
    "2) Find out how to load modules on all workers via dask-ec2..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Local:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 20.9 s, sys: 0 ns, total: 20.9 s\n",
      "Wall time: 20.6 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, error_score='raise',\n",
       "       estimator=SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape=None, degree=3, gamma='auto', kernel='rbf',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False),\n",
       "       fit_params={}, iid=True, n_jobs=1,\n",
       "       param_grid={'C': [0.0001, 1, 10000.0], 'gamma': [0.001, 1, 1000.0], 'class_weight': [None, 'balanced']},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Fit with scikit-learn\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_space = {'C': [1e-4, 1, 1e4],\n",
    "               'gamma': [1e-3, 1, 1e3],\n",
    "               'class_weight': [None, 'balanced']}\n",
    "\n",
    "model = SVC(kernel='rbf')\n",
    "\n",
    "digits = load_digits()\n",
    "\n",
    "search = GridSearchCV(model, param_space, cv=3)\n",
    "%time search.fit(digits.data, digits.target)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Via DASK: (note it takes longer as the two small servers on AWS are basically < than my laptop!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 84 ms, sys: 20 ms, total: 104 ms\n",
      "Wall time: 25.6 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cache_cv=True, cv=3, error_score='raise',\n",
       "       estimator=SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape=None, degree=3, gamma='auto', kernel='rbf',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False),\n",
       "       iid=True, n_jobs=-1,\n",
       "       param_grid={'C': [0.0001, 1, 10000.0], 'gamma': [0.001, 1, 1000.0], 'class_weight': [None, 'balanced']},\n",
       "       refit=True, return_train_score=True, scheduler=None, scoring=None)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from dask.distributed import Client\n",
    "e = Client('54.194.173.226:8786') #different to example output above as I've restarted DASK\n",
    "\n",
    "# Fit with dask-searchcv\n",
    "from dask_searchcv import GridSearchCV\n",
    "\n",
    "param_space = {'C': [1e-4, 1, 1e4],\n",
    "               'gamma': [1e-3, 1, 1e3],\n",
    "               'class_weight': [None, 'balanced']}\n",
    "\n",
    "model = SVC(kernel='rbf')\n",
    "\n",
    "digits = load_digits()\n",
    "\n",
    "search = GridSearchCV(model, param_space, cv=3)\n",
    "%time search.fit(digits.data, digits.target)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#list of modules I'll need (incomplete)\n",
    "#!conda install scikit-learn -y\n",
    "#!conda install dask distributed -y\n",
    "#!conda install dask-searchcv -c conda-forge -y\n",
    "#!pip install paramz\n",
    "#!pip install matplotlib"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
