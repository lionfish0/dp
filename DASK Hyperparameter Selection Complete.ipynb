{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DASK DP hyperparameter selection\n",
    "\n",
    "Note: This notebook is intended to be run on the DASK scheduler.\n",
    "\n",
    "The cross-validation structure is quite complicated, to explains;\n",
    "\n",
    "- We split the data into training/test sets (normal cross-validation); for each split we want the probability of selecting each parameter configuration, and the score if that parameter selection is used.\n",
    "- The probability of selecting that configuration is computed purely from the training data (to avoid peeking) [see lines 172-175 for the relevant calls]. Within each training data set two seperate calls are made to GridSearchCV. The first [line 109] gets the sensitivities of each fold/parameter-set of the SSE. The second [line 120] gets the negative SSE of each fold/parameter-set. We can then use these two to compute probabilities of selecting each parameter configuration using the exponential mechanism.\n",
    "- The RMSE is computed on the same train/test splits (but in a seperate call, [on line 142]).\n",
    "- At the end of the computation we are left with RMSEs and probabilities, we are then able to find the unbias expected RMSE.\n",
    "\n",
    "### TODO\n",
    "\n",
    "- Test and check all this code.\n",
    "- Add unnormalisation step at end.\n",
    "- Currently passing kernel objects as points in the parameter grid causes a pickle error inside DASK. Instead I've added parameters to the class to allow lengthscale and variance to be changed. Ideally however we want to have a kernel passed to allow this class to be more flexible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Install libraries if necessary on workers etc.\n",
    "\n",
    "from dask.distributed import Client\n",
    "client = Client('127.0.0.1:8786')\n",
    "import os\n",
    "import distributed\n",
    "\n",
    "runlist = ['pip install -U pip','sudo apt install libgl1-mesa-glx -y','conda update scipy -y','pip install git+https://github.com/sods/paramz.git','pip install git+https://github.com/SheffieldML/GPy.git','pip install git+https://github.com/lionfish0/dp4gp.git','conda install dask-searchcv -c conda-forge -y']\n",
    "\n",
    "for item in runlist:\n",
    "    print(\"Installing '%s' on workers...\" % item)\n",
    "    client.run(os.system,item)\n",
    "    print(\"Installing '%s' on scheduler...\" % item)\n",
    "    client.run_on_scheduler(os.system,item)    \n",
    "    #os.system(item) #if you need to install it locally too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " /home/mike/Documents/Research/dp4gp/dp4gp/dp4gp.py:343: RuntimeWarning:divide by zero encountered in log\n",
      " /home/mike/Documents/Research/dp4gp/dp4gp/dp4gp.py:418: RuntimeWarning:covariance is not positive-semidefinite.\n",
      " /home/mike/Documents/Research/dp4gp/dp4gp/dp4gp.py:343: RuntimeWarning:divide by zero encountered in log\n",
      " /home/mike/Documents/Research/dp4gp/dp4gp/dp4gp.py:418: RuntimeWarning:covariance is not positive-semidefinite.\n",
      " /home/mike/Documents/Research/dp4gp/dp4gp/dp4gp.py:343: RuntimeWarning:divide by zero encountered in log\n",
      " /home/mike/Documents/Research/dp4gp/dp4gp/dp4gp.py:418: RuntimeWarning:covariance is not positive-semidefinite.\n",
      " /home/mike/Documents/Research/dp4gp/dp4gp/dp4gp.py:418: RuntimeWarning:covariance is not positive-semidefinite.\n",
      " /home/mike/Documents/Research/dp4gp/dp4gp/dp4gp.py:343: RuntimeWarning:divide by zero encountered in log\n",
      " /home/mike/Documents/Research/dp4gp/dp4gp/dp4gp.py:418: RuntimeWarning:covariance is not positive-semidefinite.\n",
      " /home/mike/Documents/Research/dp4gp/dp4gp/dp4gp.py:343: RuntimeWarning:divide by zero encountered in log\n",
      " /home/mike/Documents/Research/dp4gp/dp4gp/dp4gp.py:418: RuntimeWarning:covariance is not positive-semidefinite.\n",
      " /home/mike/Documents/Research/dp4gp/dp4gp/dp4gp.py:343: RuntimeWarning:divide by zero encountered in log\n",
      " /home/mike/Documents/Research/dp4gp/dp4gp/dp4gp.py:418: RuntimeWarning:covariance is not positive-semidefinite.\n",
      " /home/mike/Documents/Research/dp4gp/dp4gp/dp4gp.py:343: RuntimeWarning:divide by zero encountered in log\n",
      " /home/mike/Documents/Research/dp4gp/dp4gp/dp4gp.py:418: RuntimeWarning:covariance is not positive-semidefinite.\n",
      " /home/mike/Documents/Research/dp4gp/dp4gp/dp4gp.py:343: RuntimeWarning:divide by zero encountered in log\n",
      " /home/mike/Documents/Research/dp4gp/dp4gp/dp4gp.py:418: RuntimeWarning:covariance is not positive-semidefinite.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.0042492   0.02870927  0.2367837   0.69135983]\n",
      " [ 0.02662131  0.20103365  0.24542001  0.69299734]\n",
      " [ 0.048709    0.43837831  0.33126626  0.84603282]\n",
      " [ 0.08270285  0.48933589  0.38952283  1.31397035]\n",
      " [ 0.19239383  1.25068521  0.42003588  1.81254158]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " /home/mike/Documents/Research/dp4gp/dp4gp/dp4gp.py:343: RuntimeWarning:divide by zero encountered in log\n",
      " /home/mike/Documents/Research/dp4gp/dp4gp/dp4gp.py:418: RuntimeWarning:covariance is not positive-semidefinite.\n",
      " /home/mike/Documents/Research/dp4gp/dp4gp/dp4gp.py:343: RuntimeWarning:divide by zero encountered in log\n",
      " /home/mike/Documents/Research/dp4gp/dp4gp/dp4gp.py:418: RuntimeWarning:covariance is not positive-semidefinite.\n",
      " /home/mike/Documents/Research/dp4gp/dp4gp/dp4gp.py:343: RuntimeWarning:divide by zero encountered in log\n",
      " /home/mike/Documents/Research/dp4gp/dp4gp/dp4gp.py:418: RuntimeWarning:covariance is not positive-semidefinite.\n",
      " /home/mike/Documents/Research/dp4gp/dp4gp/dp4gp.py:343: RuntimeWarning:divide by zero encountered in log\n",
      " /home/mike/Documents/Research/dp4gp/dp4gp/dp4gp.py:418: RuntimeWarning:covariance is not positive-semidefinite.\n",
      " /home/mike/Documents/Research/dp4gp/dp4gp/dp4gp.py:343: RuntimeWarning:divide by zero encountered in log\n",
      " /home/mike/Documents/Research/dp4gp/dp4gp/dp4gp.py:418: RuntimeWarning:covariance is not positive-semidefinite.\n",
      " /home/mike/Documents/Research/dp4gp/dp4gp/dp4gp.py:343: RuntimeWarning:divide by zero encountered in log\n",
      " /home/mike/Documents/Research/dp4gp/dp4gp/dp4gp.py:418: RuntimeWarning:covariance is not positive-semidefinite.\n",
      " /home/mike/Documents/Research/dp4gp/dp4gp/dp4gp.py:343: RuntimeWarning:divide by zero encountered in log\n",
      " /home/mike/Documents/Research/dp4gp/dp4gp/dp4gp.py:418: RuntimeWarning:covariance is not positive-semidefinite.\n",
      " /home/mike/Documents/Research/dp4gp/dp4gp/dp4gp.py:343: RuntimeWarning:divide by zero encountered in log\n",
      " /home/mike/Documents/Research/dp4gp/dp4gp/dp4gp.py:418: RuntimeWarning:covariance is not positive-semidefinite.\n",
      " /home/mike/Documents/Research/dp4gp/dp4gp/dp4gp.py:343: RuntimeWarning:divide by zero encountered in log\n",
      " /home/mike/Documents/Research/dp4gp/dp4gp/dp4gp.py:418: RuntimeWarning:covariance is not positive-semidefinite.\n",
      " /home/mike/Documents/Research/dp4gp/dp4gp/dp4gp.py:343: RuntimeWarning:divide by zero encountered in log\n",
      " /home/mike/Documents/Research/dp4gp/dp4gp/dp4gp.py:418: RuntimeWarning:covariance is not positive-semidefinite.\n",
      " /home/mike/Documents/Research/dp4gp/dp4gp/dp4gp.py:343: RuntimeWarning:divide by zero encountered in log\n",
      " /home/mike/Documents/Research/dp4gp/dp4gp/dp4gp.py:418: RuntimeWarning:covariance is not positive-semidefinite.\n",
      " /home/mike/Documents/Research/dp4gp/dp4gp/dp4gp.py:343: RuntimeWarning:divide by zero encountered in log\n",
      " /home/mike/Documents/Research/dp4gp/dp4gp/dp4gp.py:418: RuntimeWarning:covariance is not positive-semidefinite.\n",
      " /home/mike/Documents/Research/dp4gp/dp4gp/dp4gp.py:343: RuntimeWarning:divide by zero encountered in log\n",
      " /home/mike/Documents/Research/dp4gp/dp4gp/dp4gp.py:418: RuntimeWarning:covariance is not positive-semidefinite.\n",
      " /home/mike/Documents/Research/dp4gp/dp4gp/dp4gp.py:343: RuntimeWarning:divide by zero encountered in log\n",
      " /home/mike/Documents/Research/dp4gp/dp4gp/dp4gp.py:418: RuntimeWarning:covariance is not positive-semidefinite.\n",
      " /home/mike/Documents/Research/dp4gp/dp4gp/dp4gp.py:343: RuntimeWarning:divide by zero encountered in log\n",
      " /home/mike/Documents/Research/dp4gp/dp4gp/dp4gp.py:418: RuntimeWarning:covariance is not positive-semidefinite.\n",
      " /home/mike/Documents/Research/dp4gp/dp4gp/dp4gp.py:343: RuntimeWarning:divide by zero encountered in log\n",
      " /home/mike/Documents/Research/dp4gp/dp4gp/dp4gp.py:418: RuntimeWarning:covariance is not positive-semidefinite.\n",
      " /home/mike/Documents/Research/dp4gp/dp4gp/dp4gp.py:343: RuntimeWarning:divide by zero encountered in log\n",
      " /home/mike/Documents/Research/dp4gp/dp4gp/dp4gp.py:418: RuntimeWarning:covariance is not positive-semidefinite.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  1.04379140e-03   9.39256749e-03   1.46038836e-01   5.91199594e-01]\n",
      " [  3.18851435e-02   2.20208406e-01   2.07628670e-01   6.29649828e-01]\n",
      " [  4.04117234e-02   3.34054795e-01   2.47510204e-01   7.13719276e-01]\n",
      " [  8.87866119e-02   7.84130685e-01   2.83409834e-01   1.04308255e+00]\n",
      " [  2.12443756e-01   1.12550033e+00   3.40077077e-01   1.86990344e+00]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " /home/mike/Documents/Research/dp4gp/dp4gp/dp4gp.py:343: RuntimeWarning:divide by zero encountered in log\n",
      " /home/mike/Documents/Research/dp4gp/dp4gp/dp4gp.py:418: RuntimeWarning:covariance is not positive-semidefinite.\n",
      " /home/mike/Documents/Research/dp4gp/dp4gp/dp4gp.py:343: RuntimeWarning:divide by zero encountered in log\n",
      " /home/mike/Documents/Research/dp4gp/dp4gp/dp4gp.py:418: RuntimeWarning:covariance is not positive-semidefinite.\n",
      " /home/mike/Documents/Research/dp4gp/dp4gp/dp4gp.py:343: RuntimeWarning:divide by zero encountered in log\n",
      " /home/mike/Documents/Research/dp4gp/dp4gp/dp4gp.py:418: RuntimeWarning:covariance is not positive-semidefinite.\n",
      " /home/mike/Documents/Research/dp4gp/dp4gp/dp4gp.py:343: RuntimeWarning:divide by zero encountered in log\n",
      " /home/mike/Documents/Research/dp4gp/dp4gp/dp4gp.py:418: RuntimeWarning:covariance is not positive-semidefinite.\n",
      " /home/mike/Documents/Research/dp4gp/dp4gp/dp4gp.py:343: RuntimeWarning:divide by zero encountered in log\n",
      " /home/mike/Documents/Research/dp4gp/dp4gp/dp4gp.py:418: RuntimeWarning:covariance is not positive-semidefinite.\n",
      " /home/mike/Documents/Research/dp4gp/dp4gp/dp4gp.py:343: RuntimeWarning:divide by zero encountered in log\n",
      " /home/mike/Documents/Research/dp4gp/dp4gp/dp4gp.py:418: RuntimeWarning:covariance is not positive-semidefinite.\n",
      " /home/mike/Documents/Research/dp4gp/dp4gp/dp4gp.py:418: RuntimeWarning:covariance is not positive-semidefinite.\n",
      " /home/mike/Documents/Research/dp4gp/dp4gp/dp4gp.py:343: RuntimeWarning:divide by zero encountered in log\n",
      " /home/mike/Documents/Research/dp4gp/dp4gp/dp4gp.py:418: RuntimeWarning:covariance is not positive-semidefinite.\n",
      " /home/mike/Documents/Research/dp4gp/dp4gp/dp4gp.py:343: RuntimeWarning:divide by zero encountered in log\n",
      " /home/mike/Documents/Research/dp4gp/dp4gp/dp4gp.py:418: RuntimeWarning:covariance is not positive-semidefinite.\n",
      " /home/mike/Documents/Research/dp4gp/dp4gp/dp4gp.py:343: RuntimeWarning:divide by zero encountered in log\n",
      " /home/mike/Documents/Research/dp4gp/dp4gp/dp4gp.py:418: RuntimeWarning:covariance is not positive-semidefinite.\n",
      " /home/mike/Documents/Research/dp4gp/dp4gp/dp4gp.py:343: RuntimeWarning:divide by zero encountered in log\n",
      " /home/mike/Documents/Research/dp4gp/dp4gp/dp4gp.py:418: RuntimeWarning:covariance is not positive-semidefinite.\n",
      " /home/mike/Documents/Research/dp4gp/dp4gp/dp4gp.py:343: RuntimeWarning:divide by zero encountered in log\n",
      " /home/mike/Documents/Research/dp4gp/dp4gp/dp4gp.py:418: RuntimeWarning:covariance is not positive-semidefinite.\n",
      " /home/mike/Documents/Research/dp4gp/dp4gp/dp4gp.py:418: RuntimeWarning:covariance is not positive-semidefinite.\n",
      " /home/mike/Documents/Research/dp4gp/dp4gp/dp4gp.py:343: RuntimeWarning:divide by zero encountered in log\n",
      " /home/mike/Documents/Research/dp4gp/dp4gp/dp4gp.py:343: RuntimeWarning:divide by zero encountered in log\n",
      " /home/mike/Documents/Research/dp4gp/dp4gp/dp4gp.py:418: RuntimeWarning:covariance is not positive-semidefinite.\n",
      " /home/mike/Documents/Research/dp4gp/dp4gp/dp4gp.py:343: RuntimeWarning:divide by zero encountered in log\n",
      " /home/mike/Documents/Research/dp4gp/dp4gp/dp4gp.py:418: RuntimeWarning:covariance is not positive-semidefinite.\n",
      " /home/mike/Documents/Research/dp4gp/dp4gp/dp4gp.py:343: RuntimeWarning:divide by zero encountered in log\n",
      " /home/mike/Documents/Research/dp4gp/dp4gp/dp4gp.py:418: RuntimeWarning:covariance is not positive-semidefinite.\n",
      " /home/mike/Documents/Research/dp4gp/dp4gp/dp4gp.py:343: RuntimeWarning:divide by zero encountered in log\n",
      " /home/mike/Documents/Research/dp4gp/dp4gp/dp4gp.py:418: RuntimeWarning:covariance is not positive-semidefinite.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  2.48378852e-04   1.82724148e-03   1.72865198e-01   5.04249809e-01]\n",
      " [  1.66914053e-02   1.26795300e-01   2.67845278e-01   6.64986382e-01]\n",
      " [  2.03271379e-02   1.66820512e-01   3.02653537e-01   7.56399727e-01]\n",
      " [  5.73889258e-02   3.80958199e-01   3.02935611e-01   9.89294247e-01]\n",
      " [  1.88143169e-01   1.21890455e+00   5.18963556e-01   2.39145729e+00]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " /home/mike/Documents/Research/dp4gp/dp4gp/dp4gp.py:343: RuntimeWarning:divide by zero encountered in log\n",
      " /home/mike/Documents/Research/dp4gp/dp4gp/dp4gp.py:418: RuntimeWarning:covariance is not positive-semidefinite.\n",
      " /home/mike/Documents/Research/dp4gp/dp4gp/dp4gp.py:343: RuntimeWarning:divide by zero encountered in log\n",
      " /home/mike/Documents/Research/dp4gp/dp4gp/dp4gp.py:418: RuntimeWarning:covariance is not positive-semidefinite.\n",
      " /home/mike/Documents/Research/dp4gp/dp4gp/dp4gp.py:343: RuntimeWarning:divide by zero encountered in log\n",
      " /home/mike/Documents/Research/dp4gp/dp4gp/dp4gp.py:418: RuntimeWarning:covariance is not positive-semidefinite.\n",
      " /home/mike/Documents/Research/dp4gp/dp4gp/dp4gp.py:343: RuntimeWarning:divide by zero encountered in log\n",
      " /home/mike/Documents/Research/dp4gp/dp4gp/dp4gp.py:418: RuntimeWarning:covariance is not positive-semidefinite.\n",
      " /home/mike/Documents/Research/dp4gp/dp4gp/dp4gp.py:343: RuntimeWarning:divide by zero encountered in log\n",
      " /home/mike/Documents/Research/dp4gp/dp4gp/dp4gp.py:418: RuntimeWarning:covariance is not positive-semidefinite.\n",
      " /home/mike/Documents/Research/dp4gp/dp4gp/dp4gp.py:343: RuntimeWarning:divide by zero encountered in log\n",
      " /home/mike/Documents/Research/dp4gp/dp4gp/dp4gp.py:418: RuntimeWarning:covariance is not positive-semidefinite.\n",
      " /home/mike/Documents/Research/dp4gp/dp4gp/dp4gp.py:343: RuntimeWarning:divide by zero encountered in log\n",
      " /home/mike/Documents/Research/dp4gp/dp4gp/dp4gp.py:418: RuntimeWarning:covariance is not positive-semidefinite.\n",
      " /home/mike/Documents/Research/dp4gp/dp4gp/dp4gp.py:343: RuntimeWarning:divide by zero encountered in log\n",
      " /home/mike/Documents/Research/dp4gp/dp4gp/dp4gp.py:418: RuntimeWarning:covariance is not positive-semidefinite.\n",
      " /home/mike/Documents/Research/dp4gp/dp4gp/dp4gp.py:343: RuntimeWarning:divide by zero encountered in log\n",
      " /home/mike/Documents/Research/dp4gp/dp4gp/dp4gp.py:418: RuntimeWarning:covariance is not positive-semidefinite.\n",
      " /home/mike/Documents/Research/dp4gp/dp4gp/dp4gp.py:343: RuntimeWarning:divide by zero encountered in log\n",
      " /home/mike/Documents/Research/dp4gp/dp4gp/dp4gp.py:418: RuntimeWarning:covariance is not positive-semidefinite.\n",
      " /home/mike/Documents/Research/dp4gp/dp4gp/dp4gp.py:343: RuntimeWarning:divide by zero encountered in log\n",
      " /home/mike/Documents/Research/dp4gp/dp4gp/dp4gp.py:418: RuntimeWarning:covariance is not positive-semidefinite.\n",
      " /home/mike/Documents/Research/dp4gp/dp4gp/dp4gp.py:343: RuntimeWarning:divide by zero encountered in log\n",
      " /home/mike/Documents/Research/dp4gp/dp4gp/dp4gp.py:418: RuntimeWarning:covariance is not positive-semidefinite.\n",
      " /home/mike/Documents/Research/dp4gp/dp4gp/dp4gp.py:343: RuntimeWarning:divide by zero encountered in log\n",
      " /home/mike/Documents/Research/dp4gp/dp4gp/dp4gp.py:418: RuntimeWarning:covariance is not positive-semidefinite.\n",
      " /home/mike/Documents/Research/dp4gp/dp4gp/dp4gp.py:343: RuntimeWarning:divide by zero encountered in log\n",
      " /home/mike/Documents/Research/dp4gp/dp4gp/dp4gp.py:418: RuntimeWarning:covariance is not positive-semidefinite.\n",
      " /home/mike/Documents/Research/dp4gp/dp4gp/dp4gp.py:343: RuntimeWarning:divide by zero encountered in log\n",
      " /home/mike/Documents/Research/dp4gp/dp4gp/dp4gp.py:418: RuntimeWarning:covariance is not positive-semidefinite.\n",
      " /home/mike/Documents/Research/dp4gp/dp4gp/dp4gp.py:343: RuntimeWarning:divide by zero encountered in log\n",
      " /home/mike/Documents/Research/dp4gp/dp4gp/dp4gp.py:418: RuntimeWarning:covariance is not positive-semidefinite.\n",
      " /home/mike/Documents/Research/dp4gp/dp4gp/dp4gp.py:343: RuntimeWarning:divide by zero encountered in log\n",
      " /home/mike/Documents/Research/dp4gp/dp4gp/dp4gp.py:418: RuntimeWarning:covariance is not positive-semidefinite.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.25212614  0.23566723  0.29164709  0.22055954]\n",
      " [ 0.26988541  0.2265527   0.27359939  0.2299625 ]\n",
      " [ 0.31079512  0.23076745  0.29491875  0.16351868]]\n",
      "[[-0.5219669  -0.43615581 -0.26681739 -0.73298314]\n",
      " [-0.66548054 -0.77083157 -0.63909182 -0.4343488 ]\n",
      " [-1.83251219 -1.81390514 -2.01421716 -1.87843712]]\n",
      "[ 0.47387207  0.62897653  1.88931607]\n",
      "[ 0.48948081  0.62743818  1.8847679 ]\n"
     ]
    }
   ],
   "source": [
    "from dp4gp import datasets\n",
    "from dp4gp import dp4gp\n",
    "from dp4gp.utils import dp_normalise, dp_unnormalise\n",
    "import random\n",
    "import numpy as np\n",
    "import GPy\n",
    "import matplotlib.pyplot as plt\n",
    "from dp4gp import histogram\n",
    "import pandas as pd\n",
    "from dask_searchcv import GridSearchCV\n",
    "#from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score # http://scikit-learn.org/stable/developers/contributing.html#estimators\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.utils.validation import check_X_y, check_array, check_is_fitted\n",
    "from sklearn.cluster import KMeans\n",
    "%matplotlib inline\n",
    "\n",
    "#from dask.distributed import Client\n",
    "#client = Client('127.0.0.1:8786')\n",
    "\n",
    "kung = datasets.load_kung()\n",
    "\n",
    "# This is an estimator for sklearn\n",
    "class DPCloaking(BaseEstimator):\n",
    "    def __init__(self, lengthscale=None, variance=None, errorlimit=None, kern=None, sensitivity=1.0, epsilon=1.0, delta=0.01, inducing=None, noisevariance=None, getxvalfoldsensitivities=False):\n",
    "        \"\"\"\n",
    "        DPCloaking(lengthscale=None, variance=None, errorlimit=None, kern=None, sensitivity=1.0, epsilon=1.0, delta=0.01, inducing=None, noisevariance=None, getxvalfoldsensitivities=False)\n",
    "        \n",
    "        lengthscale=None - kernel lengthscale\n",
    "        variance=None - kernel variance\n",
    "        noisevariance=None - model gaussian white noise\n",
    "        errorlimit=None - when using the class' score function to report the SSE\n",
    "                we can set a limit on how large a single error is allowed to be.\n",
    "        kern=None - a GPy kernel (needs to have lengthscales and variances)\n",
    "        sensitivity=1.0 - the amount one output can change\n",
    "        epsilon=1.0, delta=0.01 - DP parameters\n",
    "        inducing = None - locations of inducing points, default to None - not using inducing points.\n",
    "        getxvalfoldsensitivities=False - the score method will return the sum-squared-error\n",
    "                of each fold by default, but if this is set to true it will return the\n",
    "                sensitivity of the SSE caused by a perturbed point being in the training data. \n",
    "        \"\"\"\n",
    "        \n",
    "        self.errorlimit = errorlimit\n",
    "        self.kern = kern\n",
    "        self.sensitivity = sensitivity\n",
    "        self.epsilon = epsilon\n",
    "        self.delta = delta\n",
    "        self.inducing = inducing\n",
    "        self.getxvalfoldsensitivities = getxvalfoldsensitivities\n",
    "\n",
    "        if lengthscale is not None:\n",
    "            self.kern.lengthscale = lengthscale\n",
    "        if variance is not None:\n",
    "            self.kern.variance = variance\n",
    "        if noisevariance is not None:\n",
    "            self.noisevariance = noisevariance\n",
    "        else:\n",
    "            self.noisevariance = 1.0\n",
    "        \n",
    "    def fit(self, X, y, **kwargs):\n",
    "        \"\"\"\n",
    "        fit(X, y)\n",
    "        \n",
    "        Create the GPy model using the data in X and y, and the hyperparameters etc set by\n",
    "        the constructor (or subsequently during a grid search)\n",
    "        \"\"\"\n",
    "        self.kern.lengthscale = self.lengthscale\n",
    "        self.kern.variance = self.variance\n",
    "        if self.inducing is None:\n",
    "            self.model = GPy.models.GPRegression(X,y,self.kern,normalizer=None)\n",
    "            self.model.Gaussian_noise = self.noisevariance\n",
    "            self.dpgp = dp4gp.DPGP_cloaking(self.model,self.sensitivity,self.epsilon,self.delta)\n",
    "        else:\n",
    "            if isinstance(self.inducing, list):\n",
    "                inducinglocs = self.inducing\n",
    "            else:\n",
    "                inducinglocs = KMeans(n_clusters=self.inducing, random_state=0).fit(X).cluster_centers_\n",
    "            self.model = GPy.models.SparseGPRegression(X,y,self.kern,normalizer=None,Z=inducinglocs)\n",
    "            self.model.Gaussian_noise = self.noisevariance\n",
    "            self.dpgp = dp4gp.DPGP_inducing_cloaking(self.model,self.sensitivity,self.epsilon,self.delta)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X, Nattempts=2, Nits=5): #todo set Nits back to a larger value (e.g. 100)\n",
    "        \"\"\"\n",
    "        predict(X,Nattempts=2, Nits=100)\n",
    "        \n",
    "        make predictions of the outputs, y, given inputs X.\n",
    "        \"\"\"\n",
    "        ypred,_,_= self.dpgp.draw_prediction_samples(X,Nattempts=Nattempts,Nits=Nits)\n",
    "        return ypred\n",
    "    \n",
    "    def score(self, X, y=None):\n",
    "        \"\"\"\n",
    "        score(X,y=None)\n",
    "        \n",
    "        Return the (truncated) negative SSE or part of the sensitivity of the SSE\n",
    "        \n",
    "        If self.getxvalfoldsensitivities is True then, this isn't the actual score.\n",
    "        Instead it returns the sensitivity of the sum squared error if the perturbed\n",
    "        point lies in the training data.\n",
    "        \n",
    "        If it's false this returns the truncated NEGATIVE Sum Squared Error\n",
    "        \n",
    "            Note on truncation: we have also put a bound on the error of\n",
    "            one data point - it must lie within errorlimit of the actual\n",
    "            prediction. If it doesn't then the value is truncated.\n",
    "\n",
    "            Note: This score is used to select the hyperparameter config\n",
    "            used later in the cross-validation procedure. The truncation\n",
    "            will therefore not cause an underestimate in the final score.\n",
    "            The truncation allows us to put a bound on the sensitivity of\n",
    "            the SSE.\n",
    "        \"\"\"        \n",
    "        if self.getxvalfoldsensitivities:          \n",
    "            C = self.dpgp.get_C(X) #NumTest x NumTrain\n",
    "            #we sum over the square of this (times the sensitivity) to get,\n",
    "            #             |d c_kj|_2^2\n",
    "            #we then find the largest value of this to get,\n",
    "            #             d^2 max_j |c_kj|_2^2\n",
    "            #and this we return, giving the largest effect a training\n",
    "            #point can have on the SSE.\n",
    "            return np.max(np.sum((self.sensitivity * C)**2,0))\n",
    "        else:          \n",
    "            errors = (y-self.predict(X))\n",
    "            errors[errors>self.errorlimit] = self.errorlimit\n",
    "            errors[errors<-self.errorlimit] = -self.errorlimit\n",
    "            return -np.sum((y-self.predict(X))**2)\n",
    "\n",
    "def getprobabilities(X,y,p_grid, cv):\n",
    "    \"\"\"\n",
    "    getprobabilities(X,y,p_grid, cv)\n",
    "    \n",
    "    - X and y: Inputs and outputs\n",
    "    - p_grid: grid of parameters to search over\n",
    "    - cv: Number of cross-validation folds (this is different from the outer number of x-val folds)\n",
    "    \n",
    "    Gets the probability of picking each of the options provided by p_grid given the data in X and y.\n",
    "    The algorithm is as follows:\n",
    "     - Find the sensitivity of the SSE for each parameter-values\n",
    "     - Find the SSE of each parameter-values\n",
    "     - Find the probability of selecting those parameter-values\n",
    "    \"\"\"\n",
    "    kern = GPy.kern.RBF(2.0,lengthscale=25.0,variance=1.0)\n",
    "    errorlimit = ac_sens*4.0\n",
    "   \n",
    "    ####find sensitivity of the SSE (for each param combo)\n",
    "    #this call gets the sensivities not the scores:\n",
    "    #TODO This probably should be done locally as it's quick.\n",
    "    clf = GridSearchCV(estimator=DPCloaking(sensitivity=ac_sens, inducing=4, getxvalfoldsensitivities=True, kern=kern, errorlimit=errorlimit), param_grid=p_grid, cv = cv)\n",
    "    clf.fit(X,y)\n",
    "\n",
    "    nparamcombos = len(clf.cv_results_['mean_test_score'])\n",
    "    temp_sens = np.zeros([clf.cv,nparamcombos])\n",
    "    for k in range(clf.cv):\n",
    "        temp_sens[k,:] = clf.cv_results_['split%d_test_score' % k]\n",
    "    #sensitivity of the sum squared error:\n",
    "    print(np.sort(temp_sens,axis=0))\n",
    "    sse_sens = ac_sens**2 + 2*ac_sens*errorlimit + ac_sens**2*np.max(np.sum(np.sort(temp_sens,axis=0)[0:clf.cv-1,:],0))\n",
    "\n",
    "    ####find the SSE (for each param combo)\n",
    "    clf = GridSearchCV(estimator=DPCloaking(sensitivity=ac_sens, inducing=4, getxvalfoldsensitivities=False, kern=kern, errorlimit=errorlimit), param_grid=p_grid, cv = cv)\n",
    "    clf.fit(X,y)\n",
    "\n",
    "    nparamcombos = len(clf.cv_results_['mean_test_score'])\n",
    "    temp_scores = np.zeros([clf.cv,nparamcombos])\n",
    "    for k in range(clf.cv):\n",
    "        temp_scores[k,:] = clf.cv_results_['split%d_test_score' % k]\n",
    "    scores = np.sum(temp_scores,0)\n",
    "\n",
    "    ####compute the probability of selecting that param combo using the exponential mechanism\n",
    "    selection_epsilon = 1\n",
    "    param_probabilities = np.exp(selection_epsilon*scores / (2*sse_sens))\n",
    "    param_probabilities = param_probabilities / np.sum(param_probabilities)\n",
    "    \n",
    "    return param_probabilities\n",
    "\n",
    "def getscores(X,y,p_grid,cv):\n",
    "    \"\"\"\n",
    "    Compute the negative RMSE of each of the fold/param combos\n",
    "    \"\"\"\n",
    "    kern = GPy.kern.RBF(2.0,lengthscale=25.0,variance=1.0)\n",
    "\n",
    "    clf = GridSearchCV(estimator=DPCloaking(sensitivity=ac_sens, inducing=4, getxvalfoldsensitivities=False, kern=kern), scoring='neg_mean_squared_error', param_grid=p_grid, cv = cv)\n",
    "    clf.fit(X,y)\n",
    "\n",
    "    nparamcombos = len(clf.cv_results_['mean_test_score'])\n",
    "    scores = np.zeros([clf.cv,nparamcombos])\n",
    "    for k in range(clf.cv):\n",
    "        scores[k,:] = clf.cv_results_['split%d_test_score' % k]\n",
    "    return scores\n",
    "\n",
    "####Set up data and parameter search grid\n",
    "sensitivity = 100.0\n",
    "y,ac_sens,norm_params = dp_normalise(kung[kung[:,3]==0,0:1],sensitivity)\n",
    "X = kung[kung[:,3]==0,1:3]\n",
    "epsilon = 1.0\n",
    "delta = 0.01\n",
    "cv = 3\n",
    "p_grid = {\"lengthscale\":[], 'variance':[]}#, 'noisevariance':[]}\n",
    "for ls in 5.0**np.arange(0,2):\n",
    "    p_grid[\"lengthscale\"].append(ls)\n",
    "for v in 5.0**np.arange(-1,1):\n",
    "    p_grid[\"variance\"].append(v)\n",
    "    \n",
    "####Get the -RMSE for each fold/param-combo\n",
    "scores = getscores(X,y,p_grid,cv)\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "\n",
    "kf = KFold(n_splits=cv)\n",
    "probabilities = []\n",
    "for train_index, test_index in kf.split(X):\n",
    "    X_train = X[train_index]\n",
    "    y_train = y[train_index]\n",
    "    probabilities.append(getprobabilities(X_train,y_train,p_grid,5))\n",
    "    \n",
    "print(np.array(probabilities))\n",
    "print(scores)\n",
    "print(np.sum(probabilities*-scores,1))\n",
    "print(np.mean(-scores,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.60348814, -0.50939675, -0.65877968, -0.27725199],\n",
       "       [-0.65522194, -0.95756242, -0.58511046, -0.98094521],\n",
       "       [-1.8343872 , -1.9017283 , -1.55337318, -2.03818546]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([ 0.2780088 ,  0.2651039 ,  0.25994567,  0.19694163]),\n",
       " array([ 0.26712682,  0.23700197,  0.31964679,  0.17622442]),\n",
       " array([ 0.30261986,  0.27097601,  0.26606621,  0.16033791])]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
