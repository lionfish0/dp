{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vector Alternative\n",
    "\n",
    "Previously I used Hall's method for applying DP to functions.\n",
    "\n",
    "I found a bound which applied to any test point $x_*$. I then tried to alter this to just consider the greatest bound for the test points used (i.e. calculate the exact bound for each test point, and use that to calculate $\\Delta$). But it turns out I can't use as the sensitivity the test point with the highest sensitivity as I need to bound the whole function. \n",
    "\n",
    "Note: I've not investigated the effect of using a modified covariance function much: it seems to behave quite strangely. The problem is its interaction with the sensitivity.\n",
    "\n",
    "I've looked into the direction of the noise added, and compared it to the effect of modifying a training point. I think sensitivity needs to be quite high because the noise added (sampled from the *prior*) is not necessarily in the direction a perturbation in a training output would cause.\n",
    "\n",
    "Consider the simple case of two training and two test points as in this figure;\n",
    "\n",
    "#### Figure 1\n",
    "<img src=\"demo_problem_with_standard_fn_noise.png\" />\n",
    "\n",
    "Put simply: We're interested in the effect of changing the training outputs on the test outputs.\n",
    "\n",
    "Using the method developed in the earlier version of the paper we add noise with correlations defined by the default kernel (i.e. from the prior). In the following figure the shaded contour plot shows the distribution of DP noise added to two nearby test points. The two axes are for the two test points above (x-axis left point, y-axis right point). \n",
    "\n",
    "The lined contours is the same noise, but for when the training data has been perturbed (as in the above figure): The y-axis represents the right test point, and, as this has moved due to the perturbation, the gaussian has shifted upwards. Because the other training point wasn't affected the gaussian has not shiften along the x-axis.\n",
    "\n",
    "#### Figure 2\n",
    "<img src=\"problem.png\" />\n",
    "\n",
    "If the other training point were to move, the gaussian would be shifted rightward and *downward* (think about how the right test point would move if the left training point moved up).\n",
    "\n",
    "In this case I've reduced the $\\Delta$ of the DP so the two gaussians don't overlap too much. But for DP to exist the $\\Delta$ must be large enough that the two distributions overlap sufficiently. It seems to me that the direction of the Gaussian noise is wrong, and that it needs to point in the direction which means the $\\Delta$ is as small as possible.\n",
    "\n",
    "To this end, I went back to an earlier step in the Hall paper, for a bound on a vector rather than a function. This means, I think, I can add whatever gaussian I want (the covariance of the gaussian doesn't have to match the kernel).\n",
    "\n",
    "Copied from the paper:\n",
    "\n",
    "> Proposition 3: Suppose that, for a positive definite symmetric matrix $M \\in \\mathbb{R}^{d \\times d}$, the family of vectors $\\{\\mathbf{v}_D : D \\in \\mathcal{D}\\} \\subset \\mathbb{R}^d$\n",
    ">\n",
    "> $$sup_{D \\sim {D'}} ||M^{-1/2} (\\mathbf{v}_D - \\mathbf{v}_{D'})||_2 \\leq \\Delta$$\n",
    ">\n",
    "> Then the randomized algorithm which, for input database D outputs\n",
    ">\n",
    "> $$\\tilde{\\mathbf{v}_D} = \\mathbf{v}_D + \\frac{c(\\delta)}{\\alpha}Z$$\n",
    ">\n",
    "> where $$Z \\sim \\mathcal{N}_d(0,M)$$\n",
    ">\n",
    "> achieves $(\\varepsilon, \\delta)$-DP whenever,\n",
    ">\n",
    "> $$c(\\delta) \\geq \\sqrt{2 log \\frac{2}{\\delta}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to construct $M$ so that it has greatest covariance in those directions most affected by changes in test points.\n",
    "\n",
    "We build the matrix $K$ to represent the covariance between all training points, with the sample variance built in, i.e. $K_{i,i} = k(x_i,x_i) + \\sigma^2$.\n",
    "\n",
    "A test point $x_*$ has mean calculated as: $\\mathbf{k}_* K^{-1} \\mathbf{y}$ where $\\mathbf{k}_*$ is the covariance between test point $x_*$ and all the training points.\n",
    "\n",
    "We build another matrix $C$, the cloaking-matrix, or sensitivity matrix. This describes how much each test point will be altered for a unit change in each training point.\n",
    "\n",
    "$$C = K_{*N} K^{-1}$$\n",
    "\n",
    "where $K_{*N}$ is the covariance between all test and training points, and $K$ is the covariance between training points.\n",
    "\n",
    "One column, i, of $C$, represents the effect of training point $x_i$'s perturbation by 1 on all test points. Of interest here then is the variance and covariance of all the test points when manipulating training point $i$.\n",
    "\n",
    "We can find a covariance matrix which describes these simply by considering the total set of test point mean predictions.\n",
    "\n",
    "$\\hat{\\mathbf{y}} = \\mathbf{K}_* K^{-1} \\mathbf{y}$ where $\\mathbf{k}_*$\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
