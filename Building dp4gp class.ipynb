{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "warning in stationary: failed to import cython module: falling back to numpy\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import GPy\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Methods for combining differential privacy with Gaussian Processes\n",
    "\n",
    "import GPy\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "import sys\n",
    "import scipy\n",
    "from scipy.stats import multivariate_normal\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "class DPGP(object):\n",
    "    \"\"\"(epsilon,delta)-Differentially Private Gaussian Process predictions\"\"\"\n",
    "    \n",
    "    def __init__(self,model,sens,epsilon,delta):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "            model = Pass a GPy model object\n",
    "            sens = data sensitivity (how much can one output value vary due to one person\n",
    "            epsilon = epsilon (DP parameter)\n",
    "            delta = delta (DP parameter) [probability of providing DP]\n",
    "            \n",
    "        \"\"\"\n",
    "        self.model = model\n",
    "        self.sens = sens\n",
    "        self.epsilon = epsilon\n",
    "        self.delta = delta\n",
    "        assert epsilon<=1, \"The proof in Hall et al. 2013 is restricted to values of epsilon<=1.\"\n",
    "    \n",
    "    def draw_prediction_samples(self,Xtest,N=1):\n",
    "        GPymean, covar = self.model.predict(Xtest)\n",
    "        mean, noise, _ = self.draw_noise_samples(Xtest,N)\n",
    "        #TODO: In the long run, remove DP4GP's prediction code and just use GPy's\n",
    "        #print GPymean-mean\n",
    "        assert np.max(GPymean-mean)<1e-3, \"DP4GP code's posterior mean prediction differs from GPy's\"\n",
    "        return mean + noise.T\n",
    "    \n",
    "    def plot(self):\n",
    "        raise NotImplementedError #need to implemet in a subclass\n",
    "        \n",
    "class DPGP_prior(DPGP):\n",
    "    \"\"\"\n",
    "    DP provided by adding a sample from the prior\n",
    "    \"\"\"\n",
    "    \n",
    "#    def __init__(self,model,sens,epsilon,delta):      \n",
    "#        super(DPGP_prior, self).__init__(model,sens,epsilon,delta)\n",
    "        \n",
    "    def calc_msense(self,A):\n",
    "        \"\"\"\n",
    "        originally returned the infinity norm*, but we've developed an improved value from\n",
    "        this norm which only cares about values of the same sign (it is assumed that\n",
    "        those of the opposite sign will work to reduce the sensitivity). We'll call\n",
    "        this the matrix_sensitivity or msense\n",
    "        * np.max(np.sum(np.abs(A),1))\n",
    "        \"\"\"\n",
    "        v1 = np.max(np.abs(np.sum(A.copy().clip(min=0),1)))\n",
    "        v2 = np.max(np.abs(np.sum((-A.copy()).clip(min=0),1)))\n",
    "        return np.max([v1,v2])\n",
    "\n",
    "    def draw_cov_noise_samples(self,test_cov,msense,N=1):        \n",
    "        \"\"\"\n",
    "        Produce differentially private noise for this covariance matrix\n",
    "        \"\"\"\n",
    "        G = np.random.multivariate_normal(np.zeros(len(test_cov)),test_cov,N)\n",
    "        noise = G*self.sens*np.sqrt(2*np.log(2/self.delta))/self.epsilon\n",
    "        noise = noise * msense\n",
    "        return np.array(noise), test_cov*msense*self.sens*np.sqrt(2*np.log(2/self.delta))/self.epsilon\n",
    "\n",
    "    def draw_noise_samples(self,Xtest,N=1):\n",
    "        raise NotImplementedError #need to implemet in a subclass\n",
    "        \n",
    "    #def draw_prediction_samples(self,Xtest,N=1):\n",
    "    #    GPymean, covar = self.model.predict(Xtest)\n",
    "    #    mean, noise, _ = self.draw_noise_samples(Xtest,N)\n",
    "    #    #TODO: In the long run, remove DP4GP's prediction code and just use GPy's\n",
    "    #    assert np.max(GPymean-mean)<1e-3, \"DP4GP code's posterior mean prediction differs from GPy's\"\n",
    "    #    return mean + noise.T\n",
    "    \n",
    "    def plot(self):\n",
    "        p = self.model.plot(legend=False)\n",
    "        xlim = p.axes.get_xlim()\n",
    "        Xtest = np.arange(xlim[0],xlim[1],(xlim[1]-xlim[0])/100.0)[:,None]\n",
    "        mu = self.draw_prediction_samples(Xtest,20)\n",
    "        plt.plot(Xtest,mu,'-k',alpha=0.3);\n",
    "    \n",
    "class DPGP_normal_prior(DPGP_prior):\n",
    "    def __init__(self,model,sens,epsilon,delta):      \n",
    "        super(DPGP_normal_prior, self).__init__(model,sens,epsilon,delta)\n",
    "        self.calc_invCov()\n",
    "        \n",
    "    def calc_invCov(self):\n",
    "        \"\"\"\n",
    "        TODO\n",
    "        \"\"\"\n",
    "        sigmasqr = self.model.Gaussian_noise.variance[0]\n",
    "        K_NN_diags = self.model.kern.Kdiag(self.model.X)\n",
    "        K_NN = self.model.kern.K(self.model.X)\n",
    "        invCov = np.linalg.inv(K_NN+sigmasqr*np.eye(K_NN.shape[0]))\n",
    "        self.invCov = invCov\n",
    "        \n",
    "    def draw_noise_samples(self,Xtest,N=1):\n",
    "        \"\"\"\n",
    "        For a given set of test points, find DP noise samples for each\n",
    "        \"\"\"\n",
    "        test_cov = self.model.kern.K(Xtest,Xtest)\n",
    "        msense = self.calc_msense(self.invCov)\n",
    "        \n",
    "        ##This code is only necessary for finding the mean (for testing it matches GPy's)\n",
    "        sigmasqr = self.model.Gaussian_noise.variance[0]\n",
    "        K_NN = self.model.kern.K(self.model.X)\n",
    "        K_Nstar = self.model.kern.K(self.model.X,Xtest)\n",
    "        mu = np.dot(np.dot(K_Nstar.T,np.linalg.inv(K_NN+sigmasqr*np.eye(K_NN.shape[0]))),self.model.Y)\n",
    "        ##\n",
    "        samps, samp_cov = self.draw_cov_noise_samples(test_cov,msense,N)\n",
    "        return mu, samps, samp_cov\n",
    "      \n",
    "    \n",
    "class DPGP_pseudo_prior(DPGP_prior):\n",
    "    def draw_noise_samples(self,Xtest,N=1):\n",
    "        \"\"\"\n",
    "        For a given set of test points, find DP noise samples for each\n",
    "        \"\"\"\n",
    "        self.model.inference_method = GPy.inference.latent_function_inference.FITC()\n",
    "        test_cov = self.model.kern.K(Xtest,Xtest)\n",
    "        sigmasqr = self.model.Gaussian_noise.variance[0]\n",
    "        K_NN_diags = self.model.kern.Kdiag(self.model.X)\n",
    "        K_NN = self.model.kern.K(self.model.X)\n",
    "        \n",
    "        K_star = self.model.kern.K(Xtest,self.model.Z.values)\n",
    "        K_NM = self.model.kern.K(self.model.X,self.model.Z.values)\n",
    "        K_MM = self.model.kern.K(self.model.Z.values)\n",
    "        invK_MM = np.linalg.inv(K_MM)\n",
    "        \n",
    "        #lambda values are the diagonal of the training input covariances minus \n",
    "        #(cov of training+pseudo).(inv cov of pseudo).(transpose of cov of training+pseudo)\n",
    "        lamb = np.zeros(len(self.model.X))\n",
    "        for i,t_in in enumerate(self.model.X):\n",
    "            lamb[i] = K_NN_diags[i] - np.dot(np.dot(K_NM[i,:].T,invK_MM),K_NM[i,:])\n",
    "\n",
    "        #this finds (\\Lambda + \\sigma^2 I)^{-1}\n",
    "        diag = 1.0/(lamb + sigmasqr) #diagonal values\n",
    "\n",
    "        #rewritten to be considerably less memory intensive (and make it a little quicker)\n",
    "        Q = K_MM + np.dot(K_NM.T * diag,K_NM)\n",
    "\n",
    "        #find the mean at each test point\n",
    "        pseudo_mu = np.dot(     np.dot(np.dot(K_star, np.linalg.inv(Q)),K_NM.T) *  diag  ,self.model.Y)\n",
    "        #un-normalise our estimates of the mean (one using the pseudo inputs, and one using normal GP regression)\n",
    "\n",
    "        #find the covariance for the two methods (pseudo and normal)\n",
    "        #K_pseudoInv is the matrix in: mu = k_* K_pseudoInv y\n",
    "        #i.e. it does the job of K^-1 for the inducing inputs case\n",
    "        K_pseudoInv = np.dot(np.linalg.inv(Q),K_NM.T) * diag\n",
    "\n",
    "        invlambplussigma = np.diag(1.0/(lamb + sigmasqr)) \n",
    "        assert (K_pseudoInv == np.dot(np.dot(np.linalg.inv(Q),K_NM.T),invlambplussigma)).all() #check our optimisation works\n",
    "\n",
    "        #find the sensitivity for the pseudo (inducing) inputs\n",
    "        pseudo_msense = self.calc_msense(K_pseudoInv)\n",
    "\n",
    "        samps, samp_cov = self.draw_cov_noise_samples(test_cov,pseudo_msense,N)\n",
    "        return pseudo_mu, samps, samp_cov    \n",
    "\n",
    "class DPGP_cloaking(DPGP):\n",
    "    \"\"\"Using the cloaking method\"\"\"\n",
    "    \n",
    "    def __init__(self,model,sens,epsilon,delta):      \n",
    "        super(DPGP_cloaking, self).__init__(model,sens,epsilon,delta)\n",
    "\n",
    "    def calcM(self,ls,cs):\n",
    "        \"\"\"\n",
    "        Find the covariance matrix, M, as the lambda weighted sum of c c^T\n",
    "        \"\"\"\n",
    "        d = len(cs[0])\n",
    "        M = np.zeros([d,d])\n",
    "        ccTs = []\n",
    "        for l,c in zip(ls,cs):        \n",
    "            ccT = np.dot(c,c.T)\n",
    "            #print c,ccT,l,M\n",
    "            M = M + l*ccT       \n",
    "            ccTs.append(ccT)\n",
    "        return M\n",
    "\n",
    "    def L(self,ls,cs):\n",
    "        \"\"\"\n",
    "        Find L = -log |M| + sum(lambda_i * (1-c^T M^-1 c))\n",
    "        \"\"\"\n",
    "        M = self.calcM(ls,cs)\n",
    "        Minv = np.linalg.pinv(M)\n",
    "        t = 0\n",
    "        for l,c in zip(ls,cs):        \n",
    "            t += l*(1-np.dot(np.dot(c.T,Minv),c))[0,0]\n",
    "\n",
    "        return (np.log(np.linalg.det(Minv)) + t)\n",
    "        #return t\n",
    "        \n",
    "    def dL_dl(self,ls,cs):\n",
    "        \"\"\"\n",
    "        Find the gradient dL/dl_j\n",
    "        \"\"\"\n",
    "        M = self.calcM(ls,cs)\n",
    "        Minv = np.linalg.pinv(M)            \n",
    "        grads = np.zeros(len(ls))    \n",
    "        for j in range(len(cs)):        \n",
    "            grads[j] = -np.trace(np.dot(Minv,np.dot(cs[j],cs[j].T)))     \n",
    "        return np.array(grads)+1\n",
    "    \n",
    "    def findLambdas_grad(self, cs, maxit=700):\n",
    "        \"\"\"\n",
    "        Gradient descent to find the lambda_is\n",
    "\n",
    "        Parameters:\n",
    "            cs = list of column vectors (these are the gradients of df*/df_i)\n",
    "\n",
    "        Returns:\n",
    "            ls = vector of lambdas\n",
    "\n",
    "        \"\"\"\n",
    "        ls = np.ones(len(cs))*0.7\n",
    "        lr = 0.05 #learning rate\n",
    "        for it in range(maxit): \n",
    "            lsbefore = ls.copy()\n",
    "            delta_ls = -self.dL_dl(ls,cs)*lr\n",
    "            ls =  ls + delta_ls\n",
    "            ls[ls<0] = 0\n",
    "            #lr*=0.995\n",
    "            if np.max(np.abs(lsbefore-ls))<1e-5:\n",
    "                return ls\n",
    "        print \"Stopped before convergence\"\n",
    "        return ls\n",
    "    \n",
    "    def findLambdas_scipy(self,cs, maxit=1000):\n",
    "        \"\"\"\n",
    "        Find optimum value of lambdas, start optimiser with random lambdas.\n",
    "        \"\"\"\n",
    "        #ls = np.ones(len(cs))*0.7\n",
    "        ls = np.random.rand(len(cs))+0.5\n",
    "        cons = ({'type':'ineq','fun':lambda ls:np.min(ls)})\n",
    "        #cons = []\n",
    "        #for i in range(len(ls)):\n",
    "        #    cons.append({'type':'ineq', 'fun':lambda ls:ls[i]})\n",
    "        res = minimize(self.L, ls, args=(cs), method='SLSQP', options={'ftol': 1e-12, 'disp': True, 'maxiter': maxit}, constraints=cons, jac=self.dL_dl)\n",
    "        ls = res.x \n",
    "        #print ls\n",
    "        return ls\n",
    "    \n",
    "    def findLambdas_repeat(self,cs,Nattempts=7,Nits=1000):\n",
    "        \"\"\"\n",
    "        Call findLambdas repeatedly with different start lambdas, to avoid local minima\n",
    "        \"\"\"\n",
    "        bestLogDetM = np.Inf\n",
    "        bestls = None        \n",
    "        for it in range(Nattempts):\n",
    "            ls = self.findLambdas_grad(cs,Nits)\n",
    "            if np.min(ls)<-0.01:\n",
    "                continue\n",
    "            M = self.calcM(ls,cs)\n",
    "            logDetM = np.log(np.linalg.det(M))\n",
    "            if logDetM<bestLogDetM:\n",
    "                bestLogDetM = logDetM\n",
    "                bestls = ls.copy()\n",
    "        if bestls is None:\n",
    "            print \"Failed to find solution\"\n",
    "        #print bestls\n",
    "        return bestls\n",
    "    \n",
    "    def calcDelta(self,ls,cs):\n",
    "        \"\"\"\n",
    "        We want to find a \\Delta that satisfies sup{D~D'} ||M^-.5(v_D-v_D')||_2 <= \\Delta\n",
    "        this is equivalent to finding the maximum of our c^T M^-1 c.\n",
    "        \"\"\"\n",
    "        M = self.calcM(ls,cs)\n",
    "        Minv = np.linalg.pinv(M)\n",
    "        maxcMinvc = -np.Inf\n",
    "        for l,c in zip(ls,cs):\n",
    "            cMinvc = np.dot(np.dot(c.transpose(), Minv),c)\n",
    "            if cMinvc>maxcMinvc:\n",
    "                maxcMinvc = cMinvc\n",
    "        return maxcMinvc\n",
    "\n",
    "    def checkgrad(self,ls,cs):\n",
    "        \"\"\"\n",
    "        Gradient check (test if the analytical derivative dL/dlambda_i almost equals the numerical one)\"\"\"\n",
    "        approx_dL_dl = []\n",
    "        d = 0.0001\n",
    "        for i in range(len(ls)):\n",
    "            delta = np.zeros_like(ls)\n",
    "            delta[i]+=d\n",
    "            approx_dL_dl.append(((self.L(ls+delta,cs)-self.L(ls-delta,cs))/(2*d)))\n",
    "        approx_dL_dl = np.array(approx_dL_dl)\n",
    "\n",
    "        print \"Value:\"\n",
    "        print self.L(ls,cs)\n",
    "        print \"Approx\"\n",
    "        print approx_dL_dl\n",
    "        print \"Analytical\"\n",
    "        print self.dL_dl(ls,cs)\n",
    "        print \"Difference\"\n",
    "        print approx_dL_dl-self.dL_dl(ls,cs)\n",
    "        print \"Ratio\"\n",
    "        print approx_dL_dl/self.dL_dl(ls,cs)\n",
    "\n",
    "    def draw_noise_samples(self,Xtest,N=1):\n",
    "        \"\"\"\n",
    "        Provide N samples of the DP noise\n",
    "        \"\"\"\n",
    "        sigmasqr = self.model.Gaussian_noise.variance[0]\n",
    "        K_NN = self.model.kern.K(self.model.X)\n",
    "        K_NNinv = np.linalg.inv(K_NN+sigmasqr*np.eye(K_NN.shape[0]))\n",
    "        K_Nstar = self.model.kern.K(Xtest,self.model.X)\n",
    "        C = np.dot(K_Nstar,K_NNinv)\n",
    "\n",
    "        cs = []\n",
    "        for i in range(C.shape[1]):\n",
    "            cs.append(C[:,i][:,None])\n",
    "        \n",
    "        ls = self.findLambdas_repeat(cs,1,1000)\n",
    "        M = self.calcM(ls,cs)\n",
    "        \n",
    "        c = np.sqrt(2*np.log(2/self.delta))\n",
    "        Delta = self.calcDelta(ls,cs)\n",
    "        #in Hall13 the constant below is multiplied by the samples,\n",
    "        #here we scale the covariance by the square of this constant.\n",
    "        sampcov = ((self.sens*c*Delta/self.epsilon)**2)*M\n",
    "        samps = np.random.multivariate_normal(np.zeros(len(sampcov)),sampcov,N)\n",
    "        \n",
    "        ###This code is only necessary for finding the mean\n",
    "        mu = np.dot(C,self.model.Y)\n",
    "        ###\n",
    "        return mu, samps, sampcov\n",
    "    \n",
    "    def plot(self,N=20):\n",
    "        p = self.model.plot(legend=False)\n",
    "        xlim = p.axes.get_xlim()        \n",
    "        Xtest = np.arange(xlim[0],xlim[1],(xlim[1]-xlim[0])/100.0)[:,None]\n",
    "        mu = self.draw_prediction_samples(Xtest,N)\n",
    "        plt.plot(Xtest,mu,'-k',alpha=0.3)\n",
    "        \n",
    "class Test_DPGP_cloaking(object):\n",
    "    def test(self):\n",
    "        sens = 2\n",
    "        eps = 1.0\n",
    "        delta = 0.01\n",
    "        trainX = np.random.randn(50,1)*10 \n",
    "        #trainX = np.arange(0,10,0.2)[:,None]\n",
    "        trainy = np.sin(trainX)+np.random.randn(len(trainX),1)*0.5\n",
    "        Xtest = np.arange(0,10,2)[:,None] #0.2\n",
    "\n",
    "        mod = GPy.models.GPRegression(trainX,trainy)\n",
    "        mod.Gaussian_noise = 0.5**2\n",
    "        mod.rbf.lengthscale = 1.0\n",
    "        dpgp = DPGP_cloaking(mod,sens,eps,delta)\n",
    "        mean, noise, sampcov = dpgp.draw_noise_samples(Xtest,2)\n",
    "\n",
    "        largest_notDP = -np.Inf\n",
    "        #dpgp, noise, sampcov = get_noise(trainX,trainy,Xtest,sens,eps,delta)\n",
    "        for perturb_index in range(50): \n",
    "            mod = GPy.models.GPRegression(trainX,trainy)\n",
    "            mod.Gaussian_noise = 0.5**2\n",
    "            mod.rbf.lengthscale = 1.0\n",
    "            dpgp = DPGP_cloaking(mod,sens,eps,delta)\n",
    "            muA, _ = dpgp.model.predict(Xtest)\n",
    "            pert_trainy = np.copy(trainy)\n",
    "            pert_trainy[perturb_index]+=sens\n",
    "            mod = GPy.models.GPRegression(trainX,pert_trainy)\n",
    "            mod.Gaussian_noise = 0.5**2\n",
    "            mod.rbf.lengthscale = 1.0\n",
    "            dpgp = DPGP_cloaking(mod,sens,eps,delta)\n",
    "            muB, _ = dpgp.model.predict(Xtest)\n",
    "\n",
    "\n",
    "            dist = multivariate_normal(muA[:,0],sampcov)\n",
    "            dist_shift = multivariate_normal(muB[:,0],sampcov)\n",
    "            N = 200000\n",
    "            #print(\"These two numbers should be less than delta=%0.4f\" % dpgp.delta)\n",
    "            #print(\"Note epsilon = %0.4f\" % dpgp.epsilon)\n",
    "            pos = np.random.multivariate_normal(muA[:,0],sampcov,N)\n",
    "            proportion_notDP_A = np.mean( (dist.pdf(pos)/dist_shift.pdf(pos))>np.exp(dpgp.epsilon) )\n",
    "            pos = np.random.multivariate_normal(muB[:,0],sampcov,N)\n",
    "            proportion_notDP_B = np.mean( (dist_shift.pdf(pos)/dist.pdf(pos))>np.exp(dpgp.epsilon) )\n",
    "            assert proportion_notDP_A < dpgp.delta\n",
    "            assert proportion_notDP_B < dpgp.delta\n",
    "\n",
    "            largest_notDP = np.max([largest_notDP,proportion_notDP_A,proportion_notDP_B])\n",
    "        print \"The largest proportion of values exceeding the epsilon-DP constraint is %0.6f. This should be less than delta, which equals %0.6f\" % (largest_notDP, dpgp.delta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test code\n",
    "\n",
    "This empirically checks if the system is achieving $(\\varepsilon, \\delta)$-DP. Although this isn't a proof, it is probably a wise sanity check. Note that $\\varepsilon \\leq 1$ in the proof."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The largest proportion of values exceeding the epsilon-DP constraint is 0.001080. This should be less than delta, which equals 0.010000\n"
     ]
    }
   ],
   "source": [
    "t = Test_DPGP_cloaking()\n",
    "t.test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trainX = np.arange(0,10,0.05)[:,None]\n",
    "trainy = np.sin(trainX)+np.random.randn(len(trainX),1)*0.5\n",
    "sens = 2\n",
    "#ystd = np.std(trainy)\n",
    "#trainy = trainy/ystd\n",
    "ystd = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(trainX,trainy);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demo of cloaking method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mod = GPy.models.GPRegression(trainX,trainy)\n",
    "mod.Gaussian_noise = 0.5**2/ystd\n",
    "mod.rbf.lengthscale = 1.0\n",
    "dpgp = DPGP_cloaking(mod,sens/ystd,1.0,0.01)\n",
    "#Xtest = np.arange(0,10,0.1)[:,None]\n",
    "#mu = dpgp.draw_prediction_samples(Xtest,20)\n",
    "dpgp.plot()\n",
    "plt.ylim([-2,2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demo of Pseudo-inputs method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mod = GPy.models.SparseGPRegression(trainX,trainy,num_inducing=3)\n",
    "mod.inference_method = GPy.inference.latent_function_inference.FITC()\n",
    "mod.set_Z(np.array([[0,2,4,6,8,10]]).T)\n",
    "mod.Gaussian_noise = 0.5**2/ystd\n",
    "mod.rbf.lengthscale = 1.0\n",
    "dpgp = DPGP_pseudo_prior(mod,10.0/ystd,100.0,0.01)\n",
    "#Xtest = np.arange(0,10,0.1)[:,None]\n",
    "#mu = dpgp.draw_prediction_samples(Xtest,20)\n",
    "dpgp.plot()\n",
    "plt.ylim([-2,2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demo of standard GP method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mod = GPy.models.GPRegression(trainX,trainy)\n",
    "mod.Gaussian_noise = 0.5**2/ystd\n",
    "mod.rbf.lengthscale = 1.0\n",
    "dpgp = DPGP_normal_prior(mod,10.0/ystd,100.0,0.01)\n",
    "#Xtest = np.arange(0,10,0.1)[:,None]\n",
    "#mu = dpgp.draw_prediction_samples(Xtest,20)\n",
    "\n",
    "dpgp.plot()\n",
    "plt.ylim([-2,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
